## 1 - 简介 Introduction

---

### 1. 并行计算 Parallel Computing

我们的计算机从最早的埃尼阿克到现在的各种超算，都是为了应用而产生的，软件和硬件相互刺激而相互进步，并行计算也是这样产生的。最早的计算机肯定不是并行的，但是可以做成多线程的，因为当时一个CPU只有一个核，所以不可能一个核同时执行两个计算，后来的应用逐步要求计算量越来越高，所以单核的计算速度也在逐步上升，后来大规模并行应用产生了，我们迫切的需要能够同时处理很多数据的机器，比如图像处理，以及处理大规模的同时访问的服务器后台。

并行计算其实设计到两个不同的技术领域：

- 计算机架构（硬件）
- 并行程序设计（软件）
  
这两个很好理解，一个是生产工具，一个用工具产生各种不同应用。

硬件主要的目标就是为软件提供更快的计算速度，更低的性能功耗比，硬件结构上支持更快的并行。软件的主要目的是使用当前的硬件压榨出最高的性能，给应用提供更稳定快速的计算结果。

传统的计算机结构一般是哈佛体系结构（后来演变出冯·诺依曼结构）主要分成三部分：

- **内存（指令内存，数据内存）**
- **中央处理单元（控制单元和算数逻辑单元）**
- **输入、输出接口**

![Computer architecture](/images/Professional%20CUDA%20C%20Programming/Computer%20architecture.png)

写并行和串行的最大区别就是，写串行程序可能不需要学习不同的硬件平台，但是写并行程序就需要对硬件有一定的了解了。

#### 1.1. 串行编程和并行编程 Sequential and Parallel Programming

- **串行编程**：把问题划分成许多的运算块，每一个运算块执行一个指定的任务
- **并行编程**：将任务分解为多个子任务，并同时在多个处理器或核心上执行这些子任务

![](/images/Professional%20CUDA%20C%20Programming/Pasted%20image%2020250123171925.png)

![](/images/Professional%20CUDA%20C%20Programming/Pasted%20image%2020250123171952.png)

#### 1.2. 并行性 Parallelism

写并行程序主要是分解任务，我们一般把一个程序看成是指令和数据的组合，当然并行也可以分为这两种：

- **指令并行**
- **数据并行**

我们的任务更加关注数据并行，所以我们的主要任务是分析数据的相关性，哪些可以并行，哪些不能不行。

我们研究的是大规模数据计算，计算过程比较单一（不同的数据基本用相同的计算过程）但是数据非常多，所以我们主要是数据并行，分析好数据的相关性，决定了我们的程序设计。CUDA非常适合数据并行

数据并行程序设计，第一步就是把数据依据线程进行划分

1. **块划分**，把一整块数据切成小块，每个小块随机的划分给一个线程，每个块的执行顺序随机

| thread | 1    | 2    | 3    | 4     | 5      |
|--------|------|------|------|-------|--------|
| block  | 1 2 3| 4 5 6| 7 8 9| 10 11 12| 13 14 15|

2. **周期划分**，线程按照顺序处理相邻的数据块，每个线程处理多个数据块，比如我们有五个线程，线程1执行块1，线程2执行块2…..线程5执行块5，线程1执行块6

| thread | 1 | 2 | 3 | 4 | 5 | 1 | 2 | 3 | 4 | 5 | 1 | 2 | 3 | 4 | 5 |
|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| block  | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10| 11| 12| 13| 14| 15|

下面是示意图，注意颜色相同的块使用的同一个线程，从执行顺序上看如下：

![data partitioning](/images/Professional%20CUDA%20C%20Programming/data%20partitioning.png)

下面是数据集上的划分上看：

![data partitioning2](/images/Professional%20CUDA%20C%20Programming/data%20partitioning2.png)

**不同的数据划分严重影响程序性能**，所以针对不同的问题和不同计算机结构，我们要通过和**理论和试验**共同来决定最终最优的数据划分。

#### 1.3. 计算机架构 Computer Architecture

划分不同计算机结构的方法有很多，广泛使用的一种被称为**佛林分类法（Flynn’s Taxonomy）**，他根据指令和数据进入CPU的方式分类，分为以下四类：

![Flynn’s Taxonomy](/images/Professional%20CUDA%20C%20Programming/Flynn’s%20Taxonomy.png)

分别以数据和指令进行分析：

- **单指令单数据SISD**：传统串行计算机，386
- **单指令多数据SIMD**：并行架构，比如向量机，所有核心指令唯一，但是数据不同，现在CPU基本都有这类的向量指令
- **多指令单数据MISD**：少见，多个指令围殴一个数据
- **多指令多数据MIMD**：并行架构，多核心，多指令，异步处理多个数据流，从而实现空间上的并行，MIMD多数情况下包含SIMD，就是MIMD有很多计算核，计算核支持SIMD

为了提高并行的计算能力，我们要从架构上实现下面这些性能提升：

- **降低延迟：** 延迟是指操作从开始到结束所需要的时间，一般用微秒计算，延迟越低越好
- **提高带宽：** 带宽是单位时间内处理的数据量，一般用 MB/s 或者 GB/s 表示
- **提高吞吐量：** 吞吐量是单位时间内成功处理的运算数量，一般用gflops来表示（十亿次浮点计算

吞吐量和延迟有一定关系，都是反应计算速度的，一个是时间除以运算次数，得到的是单位次数用的时间–延迟，一个是运算次数除以时间，得到的是**单位时间执行次数–吞吐量**。

计算机架构也可以根据内存进行划分：

- **分布式内存的多节点系统**
- **共享内存的多处理器系统**

第一个更大，通常叫做**集群**，就是一个机房好多机箱，每个机箱都有内存处理器电源等一些列硬件，通过网络互动，这样组成的就是分布式。

![clusters](/images/Professional%20CUDA%20C%20Programming/clusters.png)

第二个是**单个主板有多个处理器**，他们共享相同的主板上的内存，内存寻址空间相同，通过PCIe和内存互动。

![many-core](/images/Professional%20CUDA%20C%20Programming/many-core.png)

多个处理器可以分多片处理器，和单片多核（众核many-core），也就是有些主板上挂了好多片处理器，也有的是一个主板上就一个处理器，但是这个处理器里面有几百个核。现目前发展趋势是众核处理器，集成度更高。

GPU就属于众核系统，当然现在CPU也都是多核的了，但是它们还是有很大区别的：

- CPU 适合执行复杂的逻辑，比如多分支，其核心比较重（复杂）
- GPU 适合执行简单的逻辑，大量的数据计算，其吞吐量更高，但是核心比较轻（结构简单）

### 2. 异构计算 Heterogeneous Computing

#### 2.1. 异构架构 Heterogeneous Architecture

异构计算，**不同的计算机架构就是异构**。GPU本来的任务是做图形图像的，也就是把数据处理成图形图像，图像有个特点就是并行度很高，基本上一定距离意外的像素点之间的计算是独立的，所以属于并行任务。GPU之前是不可编程的，或者说不对用户开放的，然后就有hacker开始想办法给GPU编程，来帮助他们完成规模较大的运算，于是他们研究着色语言或者图形处理原语来和GPU对话。后来黄老板发现了这个是个新的功能啊，然后就让人开发了一套平台，CUDA，然后深度学习火了，顺带着，CUDA也火到爆炸。

x86 CPU+GPU的这种异构应该是最常见的，也有CPU+FPGA，CPU+DSP等各种各样的组合，CPU+GPU在每个笔记本或者台式机上都能找到。当然超级计算机大部分也采用异构计算的方式来提高吞吐量。  

异构架构虽然比传统的同构架构运算量更大，但是其应用复杂度更高，因为要在两个设备上进行计算，控制，传输，这些都需要人为干预，而同构的架构下，硬件部分自己完成控制，不需要人为设计。

随着时间的推移，GPU 变得越来越强大和通用，使其能够以卓越的性能和高效的能效应用于通用并行计算任务。GPU 与 CPU 之间的配合即为一种异构。

- **同构：** 使用一种或多种相同架构的处理器来执行应用程序
- **异构：** 使用一组不同的处理器架构来执行应用程序，将任务分配给最适合的架构，从而提高性能。

举例，我的服务器用的是一台 AMD 3700X CPU 加上一张 RTX3070Ti GPU 构成的服务器，GPU 插在主板的PCIe卡口上，运行程序的时候，CPU 像是一个控制者，指挥显卡完成工作后进行汇总，和下一步工作安排，所以 CPU 可以把它看做一个指挥者，主机端 host，而完成大量计算的 GPU 是我们的计算设备，设备端 device

![host and device](/images/Professional%20CUDA%20C%20Programming/host%20and%20device.png)

上面这张图能大致反应CPU和GPU的架构不同。

- 左图：一个四核CPU一般有四个ALU，ALU是完成逻辑计算的核心，也是我们平时说四核八核的核，控制单元，缓存也在片上，DRAM是内存，一般不在片上，CPU通过总线访问内存。
- 右图：GPU，绿色小方块是ALU，我们注意红色框内的部分SM，这一组ALU公用一个Control单元和Cache，这个部分相当于一个完整的多核CPU，但是不同的是ALU多了，control部分变小，可见计算能力提升了，控制能力减弱了，所以对于控制（逻辑）复杂的程序，一个GPU的SM是没办法和CPU比较的，但是对了逻辑简单，数据量大的任务，GPU更高效。并且，一个GPU有好多个SM，而且越来越多。

CPU和GPU之间通过PCIe总线连接，用于传递指令和数据，这部分也是后面要讨论的性能瓶颈之一。

一个异构应用包含两种以上架构，所以代码也包括不止一部分：

- **主机代码：** 主机端运行，被编译成主机架构的机器码。主要是控制设备，完成数据传输等控制类工作
- **设备代码：** 在设备上执行，被编译成设备架构的机器码。主要的任务就是计算

主机代码在主机端运行，被编译成主机架构的机器码，设备端的在设备上执行，被编译成设备架构的机器码，所以主机端的机器码和设备端的机器码是隔离的，自己执行自己的，没办法交换执行。**主机端代码主要是控制设备，完成数据传输等控制类工作，设备端主要的任务就是计算。**

其实当没有 GPU 的时候 CPU 也能完成这些计算，只是速度会慢很多，所以可以把GPU看成CPU的一个**加速设备**。

衡量GPU计算能力的主要靠下面两种**容量特征**：

- **CUDA核心数量（越多越好）**
- **内存大小（越大越好）**

NVIDIA目前的计算平台（不是架构）有：

- Tegra
- Geforce
- Quadro
- Tesla

每个平台针对不同的应用场景，比如Tegra用于嵌入式，Geforce是我们平时打游戏用到，Tesla是我用于服务器的，主要用于计算。上面是根据应用场景分类的几种平台。

相应的也有计算能力的性能指标:

- **峰值计算能力**：用来评估计算容量的一个指标，通常定义为每秒能处理的单精度或双精度浮点运算的数量，通常用GFlops（每秒十亿次浮点运算）或TFlops（每秒万亿次浮点运算）来表示
- **内存带宽**：从内存中读取或写入数据的比率，通常用GB/s表示

#### 2.2. 异构范例 Paradigm of Heterogeneous Computing

CPU 和 GPU 相互配合，各有所长，各有所短。低并行逻辑复杂的程序适合用 CPU，高并行逻辑简单的大数据计算适合 GPU。这种代码的编写方式能保证GPU与CPU相辅相成，从而使 CPU＋GPU 系统的计算能力得以充分利用。为了支持使用 CPU＋GPU 异构系统架构来执行应用程序，NVIDIA设计了一个被称为CUDA的编程模型。

![GPU and CPU](/images/Professional%20CUDA%20C%20Programming/GPU%20and%20CPU.png)

一个程序可以进行如下分解，串行部分和并行部分：

![Parallel and Sequence](/images/Professional%20CUDA%20C%20Programming/Parallel%20and%20Sequence.png)

CPU和GPU线程的区别：

- CPU线程是**重量级实体**，操作系统交替执行线程，线程上下文切换花销很大
- GPU线程是**轻量级的**，GPU应用一般包含成千上万的线程，多数在排队状态，线程之间切换基本没有开销。
- CPU的核被设计用来尽可能减少一个或两个线程运行时间的延迟，而GPU核则是大量线程，最大幅度提高吞吐量

#### 2.3. CUDA：一种异构计算平台 CUDA: A Platform for Heterogeneous Computing

CUDA是一种通用的并行计算平台和编程模型，不是单单指软件或者硬件，而是建立在Nvidia GPU上的一整套平台，并扩展出多语言支持

![](/images/Professional%20CUDA%20C%20Programming/Pasted%20image%2020250123212448.png)

CUDA C 是标准ANSI C语言的扩展，扩展出一些语法和关键字来编写设备端代码，而且CUDA库本身提供了大量API来操作设备完成计算。

对于API也有两种不同的层次，一种相对交高层，一种相对底层。

- CUDA驱动API
- CUDA运行时API

![](/images/Professional%20CUDA%20C%20Programming/Pasted%20image%2020250123212538.png)

驱动API是低级的API，使用相对困难，运行时API是高级API使用简单，其实现基于驱动API。这两种API是互斥的，也就是你只能用一个，两者之间的函数不可以混合调用，只能用其中的一个库。

一个CUDA应用通常可以分解为两部分：

- CPU 主机端代码
- GPU 设备端代码

CUDA nvcc编译器会自动分离你代码里面的不同部分，如图中主机代码用C写成，使用本地的C语言编译器编译，设备端代码，也就是核函数，用CUDA C编写，通过nvcc编译，链接阶段，在内核程序调用或者明显的GPU设备操作时，添加运行时库。

**注意：核函数是我们后面主要接触的一段代码，就是设备上执行的程序段**

![](/images/Professional%20CUDA%20C%20Programming/Pasted%20image%2020250123212719.png)

nvcc 是从LLVM开源编译系统为基础开发的。

![](/images/Professional%20CUDA%20C%20Programming/Pasted%20image%2020250123212743.png)

CUDA工具箱提供编译器，数学库，调试优化等工具，当然CUDA的文档是相当完善的，可以去查阅，当然在我们基本了解基础结构的情况下，直接上来看文档会变得机械化。

![](/images/Professional%20CUDA%20C%20Programming/Pasted%20image%2020250123212814.png)

### 3. CUDA Hello World

代码在文件夹 chapter01/hello_world.cu 中

```C
#include "../common/common.h"
#include <stdio.h>

/*
 * A simple introduction to programming in CUDA. This program prints "Hello
 * World from GPU! from 10 CUDA threads running on the GPU.
 */

__global__ void hello_world_GPU(void) {
    printf("Hello World from GPU!\n");
}

int main(int argc,char **argv) {
  printf("Hello World from CPU!\n");
  
  hello_world_GPU<<<1,10>>>();
  CHECK(cudaDeviceReset());    //if no this line ,it can not output hello world from gpu
  return 0;
}
```

简单介绍其中几个关键字

```
__global__
```

是告诉编译器这个是个可以在设备上执行的**核函数**

```
hello_world<<<1,10>>>();
```

这句话C语言中没有 `<<<>>>` 是对设备进行配置的参数，也是CUDA扩展出来的部分。在调用时需要用 `<<<grid, block>>>` 来指定kernel要执行的线程数量。

```
cudaDeviceReset();
```

这句话如果没有，则不能正常的运行，因为这句话包含了**隐式同步**，GPU 和 CPU 执行程序是异步的，核函数调用后成立刻会到主机线程继续，而不管GPU端核函数是否执行完毕，所以上面的程序就是GPU刚开始执行，CPU已经退出程序了，所以我们要等GPU执行完了，再退出主机线程。

一般CUDA程序分成下面这些步骤：

- 分配host内存，并进行数据初始化；
- 分配device内存，并从host将数据拷贝到device上；
- 调用CUDA的核函数在device上完成指定的运算；
- 将device上的运算结果拷贝到host上；
- 释放device和host上分配的内存。

上面的 hello world 只到第三步，没有内存交换。

### 4. CUDA C 难么  IS CUDA C PROGRAMMING DIFFICULT

CPU与GPU的编程主要区别在于对GPU架构的熟悉程度，理解机器的结构是对编程效率影响非常大的一部分，了解你的机器，才能写出更优美的代码，而目前计算设备的架构决定了局部性将会严重影响效率。  

数据局部性（数据重用，以降低内存访问的延迟）分两种：

-   **空间局部性**：指在相对较接近的存储空间内数据元素的复用
-   **时间局部性**：在相对较短的时间段内数据和/或资源的复用

这个两个性质告诉我们，当一个数据被使用，其附近的数据将会很快被使用，当一个数据刚被使用，则随着时间继续其被再次使用的可能性降低，数据可能被重复使用。

CUDA中有两个模型是决定性能的：

-   **内存层次结构**
-   **线程层次结构**

CUDA C写核函数的时候我们只写一小段串行代码，但是这段代码被成千上万的线程执行，所有线程执行的代码都是相同的，CUDA编程模型提供了一个层次化的组织线程，直接影响GPU上的执行顺序。

CUDA抽象了硬件实现：

1. 线程组的层次结构
2. 内存的层次结构
3. 障碍同步

这些都是我们后面要研究的，线程，内存是主要研究的对象，我们能用到的工具相当丰富，NVIDIA为我们提供了：

-   Nvidia Nsight集成开发环境
-   CUDA-GDB 命令行调试器
-   性能分析可视化工具
-   CUDA-MEMCHECK工具
-   GPU设备管理工具

### 5. 总结 Summary

本文从总体上粗略的介绍了CUDA这种高效的异构计算平台，并且概括了我们的将要遇到的苦难和使用到的工具，当我们学会的CUDA，那么编写高效异构计算就会像我们写串行程序一样流畅。 

---

## 参考引用

### 书籍出处

- [CUDA C编程权威指南](asset/CUDA%20&%20GPU%20Programming/CUDA%20C编程权威指南.pdf)
- [Professional CUDA C Programming](asset/CUDA%20&%20GPU%20Programming/Professional%20CUDA%20C%20Programming.pdf)

### 网页链接

- [人工智能编程 | 谭升的博客](https://face2ai.com/program-blog/)