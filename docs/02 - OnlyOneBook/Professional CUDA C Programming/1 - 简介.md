---
aliases:
  - CUDA 简介
title: CUDA 简介 Introduction
date: 2025-02-27 12:58:19
excerpt: CUDA 入门，给新手一些指引
tags:
  - CUDA
---

## 1 - CUDA 简介 Introduction

---

### 1. 并行计算 Parallel Computing

计算机自诞生以来，从最早的埃尼阿克（ENIAC）到现代的超算，其发展始终围绕着应用需求展开。软件与硬件相互促进，推动了并行计算的诞生。早期的计算机并不具备并行处理能力，但可以通过多线程技术实现一定程度的多任务处理。随着计算应用需求的增长，单核处理器的性能提升已难以满足需求，从而催生了大规模并行计算的应用。例如，图像处理、大规模数据分析和服务器后台处理等应用场景，迫切需要能够同时处理大量数据的机器。

并行计算涉及两个主要领域：

- **计算机架构（硬件**）：提升计算速度、降低功耗，并优化并行计算支持
- **并行程序设计（软件）**：充分利用硬件资源，实现高效、稳定的计算
  
传统的计算机架构通常基于哈佛体系结构（后来演变为冯·诺依曼结构），主要包括以下三部分：

- **内存（指令内存，数据内存）**
- **中央处理单元（控制单元和算数逻辑单元）**
- **输入、输出接口**

![Computer architecture](/images/Professional%20CUDA%20C%20Programming/Computer%20architecture.png)

编写并行程序与串行程序的最大区别在于，并行程序需要理解和利用底层硬件架构，而串行程序相对独立于硬件。

#### 1.1. 串行编程和并行编程 Sequential and Parallel Programming

- **串行编程**：将任务划分为多个**顺序执行**的操作，每个操作按既定顺序依次执行。
- **并行编程**：将任务拆分为多个子任务，在多个处理单元（如多核CPU、GPU）上**同时执行**，提高计算效率

![](/images/Professional%20CUDA%20C%20Programming/Pasted%20image%2020250123171925.png)

![](/images/Professional%20CUDA%20C%20Programming/Pasted%20image%2020250123171952.png)

示例代码：

```C
#include <stdio.h>

void sequential_execution() {
    for (int i = 0; i < 10; i++) {
        printf("Task %d executed sequentially\n", i);
    }
}

int main() {
    sequential_execution();
    return 0;
}
```

#### 1.2. 并行性 Parallelism

并行程序设计的核心在于任务分解，可以从这两个角度进行分类：

- **指令并行（Instruction-Level Parallelism, ILP）**
- **数据级并行（Data-Level Parallelism, DLP）**

在大规模数据计算中，数据并行尤为重要。CUDA 非常适合处理数据并行任务，通常采用以下两种数据划分方式：

1. **块划分（Block Partitioning）**：将数据划分为多个数据块，每个线程负责处理一个数据块

| thread | 1    | 2    | 3    | 4     | 5      |
|--------|------|------|------|-------|--------|
| block  | 1 2 3| 4 5 6| 7 8 9| 10 11 12| 13 14 15|

2. **周期划分（Cyclic Partitioning）**：线程按照顺序轮流处理数据块，每个线程处理多个数据块，比如我们有五个线程，线程1执行块1，线程2执行块2…..线程5执行块5，线程1执行块6

| thread | 1 | 2 | 3 | 4 | 5 | 1 | 2 | 3 | 4 | 5 | 1 | 2 | 3 | 4 | 5 |
|--------|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
| block  | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10| 11| 12| 13| 14| 15|

下面是示意图，注意颜色相同的块使用的同一个线程，从执行顺序上看如下：

![data partitioning](/images/Professional%20CUDA%20C%20Programming/data%20partitioning.png)

下面是数据集上的划分上看：

![data partitioning2](/images/Professional%20CUDA%20C%20Programming/data%20partitioning2.png)

**不同的数据划分严重影响程序性能**，所以针对不同的问题和不同计算机结构，我们要通过和**理论和试验**共同来决定最终最优的数据划分。

#### 1.3. 计算机架构 Computer Architecture

计算机架构可按**指令与数据的处理方式**进行分类，广泛使用的一种被称为**佛林分类法（Flynn’s Taxonomy）**，分为以下四类：

![Flynn’s Taxonomy](/images/Professional%20CUDA%20C%20Programming/Flynn’s%20Taxonomy.png)

- **单指令单数据SISD**：传统串行计算架构，如早期 x86 处理器
- **单指令多数据SIMD**：并行架构，比如向量机，所有核心指令唯一，但是数据不同，适用于向量处理，如现代CPU的SIMD指令集
- **多指令单数据MISD**：罕见架构，多个指令流处理同一数据流
- **多指令多数据MIMD**：并行架构，多核心，多指令，异步处理多个数据流，从而实现空间上的并行，MIMD多数情况下包含SIMD，就是MIMD有很多计算核，计算核支持SIMD

为了提高并行的计算能力，我们要从架构上实现下面这些性能提升：

- **降低延迟：** 延迟是指操作从开始到结束所需要的时间，一般用微秒计算，延迟越低越好
- **提高带宽：** 带宽是单位时间内**可以处理**的数据量，一般用 MB/s 或者 GB/s 表示
- **提高吞吐量：** 吞吐量是单位时间内**成功处理**的运算数量，一般用gflops来表示（十亿次浮点计算

吞吐量和延迟有一定关系，都是反应计算速度的，一个是时间除以运算次数，得到的是单位次数用的时间–延迟，一个是运算次数除以时间，得到的是**单位时间执行次数–吞吐量**。

计算机架构也可以根据内存进行划分：

- **分布式内存架构（Distributed Memory）**：如计算集群，多个独立计算节点通过网络通信
- **共享内存架构（Shared Memory）**：如多核处理器，多个计算单元共享同一内存地址空间

第一个更大，通常叫做**集群**，就是一个机房好多机箱，每个机箱都有内存处理器电源等一些列硬件，通过网络互动，这样组成的就是分布式。

![clusters](/images/Professional%20CUDA%20C%20Programming/clusters.png)

第二个是**单个主板有多个处理器**，他们共享相同的主板上的内存，内存寻址空间相同，通过PCIe和内存互动。

![many-core](/images/Professional%20CUDA%20C%20Programming/many-core.png)

多个处理器可以分多片处理器，和单片多核（众核many-core），也就是有些主板上挂了好多片处理器，也有的是一个主板上就一个处理器，但是这个处理器里面有几百个核。现目前发展趋势是众核处理器，集成度更高。

GPU 作为典型的众核架构，具备大规模并行计算能力，与传统 CPU 形成互补关系：

- **CPU**：适用于复杂逻辑和控制密集型任务，核心数量少但单核性能强，比如多分支，其核心比较重（复杂）
- **GPU**：适用于数据密集型任务，核心数量多但单核逻辑较简单、大量的数据计算，其吞吐量更高，但是核心比较轻（结构简单）

### 2. 异构计算 Heterogeneous Computing

#### 2.1. 异构架构 Heterogeneous Architecture

异构计算是指在同一系统中集成多种不同的计算架构（如 CPU、GPU、FPGA 或 DSP），以充分发挥各自的优势，提高计算效率。其核心思想是将计算任务按照特性进行合理划分，使得不同计算单元能够各司其职，最大化计算性能与能效比。

- **同构：** 使用一种或多种**相同架构的处理器**来执行应用程序
- **异构：** 使用一组**不同的处理器**架构来执行应用程序，将任务分配给最适合的架构，从而提高性能。

GPU 最初主要用于图形渲染，其架构设计适用于大规模并行计算。由于科学计算、人工智能等领域的任务高度并行，开发者开始利用 GPU 进行通用计算（General Purpose GPU），最终催生了 CUDA、OpenCL 等并行计算框架，使 GPU 在深度学习、数值模拟等领域得到了广泛应用。

在异构计算架构中，常见的组合包括：

- **CPU + GPU**（最常见）：CPU 负责任务调度、逻辑控制、数据管理以及串行计算任务，GPU 负责大规模并行计算，特别适用于矩阵运算、深度学习推理、科学计算等场景。典型应用：深度学习训练与推理、计算机视觉、大规模数据分析
- **CPU + FPGA**：FPGA 可针对特定任务进行硬件级优化，具有低功耗、高吞吐量的特点，适用于对时延敏感或特定算法优化的应用，如金融高频交易、5G 基站信号处理、边缘计算等
- **CPU + DSP**：DSP 专用于高效处理信号数据，特别适用于音频、视频、雷达等应用。

CPU+GPU 在每个笔记本或者台式机上都能找到，当然超级计算机大部分也采用异构计算的方式来提高吞吐量。  

异构计算的优势在于能够将计算任务分配给最合适的处理器，从而优化系统性能且比传统的同构架构运算量更大。然而因为需要管理不同处理器之间的数据传输和同步，即要在两个设备上进行计算、控制、传输，这些都需要人为干预，从而带来了更高的编程复杂度。随着时间的推移，GPU 变得越来越强大和通用，使其能够以卓越的性能和高效的能效应用于通用并行计算任务。

举例，我的服务器用的是一台 AMD 3700X CPU 加上一张 GTX1060 GPU 构成的服务器，GPU 插在主板的 PCIe 卡口上，运行程序的时候，CPU 像是一个控制者，指挥显卡完成工作后进行汇总，和下一步工作安排，所以 CPU 可以把它看做一个指挥者，**主机端 host**，而完成大量计算的 GPU 是我们的计算设备，**设备端 device**。

![host and device](/images/Professional%20CUDA%20C%20Programming/host%20and%20device.png)

上面这张图能大致反应CPU和GPU的架构不同。

- 左图：一个四核 CPU 一般有四个 ALU，ALU 是完成逻辑计算的核心，也是我们平时说四核八核的核，控制单元，缓存也在片上，DRAM 是内存，一般不在片上，CPU 通过总线访问内存。
- 右图：GPU，绿色小方块是 ALU，我们注意红色框内的部分 SM，这一组 ALU 公用一个Control 单元和 Cache，这个部分相当于一个完整的多核 CPU，但是不同的是 ALU 多了，control 部分变小，可见计算能力提升了，控制能力减弱了，所以对于控制（逻辑）复杂的程序，一个GPU的 SM 是没办法和 CPU 比较的，但是对了逻辑简单，数据量大的任务，GPU 更高效。并且，一个 GPU 有好多个 SM，而且越来越多。

#### 2.2. CPU 和 GPU 协同计算 CPU-GPU Collaboration

CPU 和 GPU 之间通过 PCIe 总线连接，用于传递指令和数据，这部分也是后面要讨论的性能瓶颈之一。一个异构应用通常包含两种及以上的计算架构，因此其代码也包含多个部分：

- **主机代码：** 主机端运行，被编译成主机架构的机器码。主要是**控制设备**，负责任务调度、数据管理和 GPU 计算的调用
- **设备代码：** 设备端执行，被编译成设备架构的机器码。主要的任务就是**计算**，专注于并行计算任务

由于 CPU 和 GPU 采用不同的架构，所以其编译后的机器码也是相互独立的，自己执行自己的，无法直接交换执行。**主机端代码主要是控制设备，完成数据传输等控制类工作，设备端主要的任务就是计算。**

在没有 GPU 的情况下，CPU 仍然可以执行相同的计算任务，但计算速度通常较慢。因此，GPU 可被视为 CPU 的**计算加速器**。

衡量GPU计算能力的主要靠下面两种**容量特征**：

- **CUDA核心数量（越多越好）**：决定 GPU 的并行计算能力
- **内存大小（越大越好）**：影响数据存储和处理能力

NVIDIA 目前的 GPU 平台（非架构）主要包括：

-   **Tegra**：面向嵌入式和移动计算应用。
-   **Geforce**：主要用于消费级图形计算和游戏（平时打游戏）。
-   **Quadro**：针对专业图形设计和计算任务。
-   **Tesla**：专注于高性能计算（HPC）和深度学习，如服务器。

此外，GPU 计算能力的评估指标还包括：

- **峰值计算能力**：用来评估计算容量的一个指标，通常定义为**每秒能处理的单精度或双精度浮点运算的数量**，通常用GFlops（每秒十亿次浮点运算）或TFlops（每秒万亿次浮点运算）来表示
- **内存带宽**：从内存中读取或写入数据的比率，通常用GB/s表示

#### 2.3. 异构范例 Paradigm of Heterogeneous Computing

CPU 与 GPU 互为补充，各具优势。CPU 适用于复杂控制逻辑和低并行度任务，而 GPU 则擅长大规模逻辑简单的大数据并行计算，如矩阵运算和深度学习推理。这种代码的编写方式能保证 GPU 与 CPU 相辅相成，从而使 CPU＋GPU 系统的计算能力得以充分利用。为支持 CPU + GPU 异构架构的应用开发，NVIDIA 设计了 **CUDA** 编程模型，使开发者能够高效地管理并行计算任务。

![GPU and CPU](/images/Professional%20CUDA%20C%20Programming/GPU%20and%20CPU.png)

程序通常可划分为**串行**和**并行**两部分：

![Parallel and Sequence](/images/Professional%20CUDA%20C%20Programming/Parallel%20and%20Sequence.png)

CPU 与 GPU 线程在执行方式上存在显著区别：

- **CPU 线程**：重量级，受限于上下文切换开销，适合低延迟、控制密集型任务
- **GPU 线程**：轻量级，一般包含成千上万的线程，多数在排队状态，线程之间**切换基本没有开销**。
- **优化策略**：CPU 侧重减少单线程执行延迟，而 GPU 通过大规模线程并发提升吞吐量

#### 2.4. CUDA：一种异构计算平台 CUDA: A Platform for Heterogeneous Computing

CUDA（Compute Unified Device Architecture）是 NVIDIA 推出的一种通用并行计算平台和编程模型。它不仅仅是一个软件库或硬件架构，而是一个完整的生态系统，建立在NVIDIA GPU 之上，支持多种编程语言（如C、C++、Python等）。CUDA 的目标是通过GPU 的强大并行计算能力，加速各种计算密集型任务，如科学计算、深度学习、图像处理等。

![](/images/Professional%20CUDA%20C%20Programming/Pasted%20image%2020250123212448.png)

CUDA C 是标准ANSI C语言的扩展，增加了一些特定的语法和关键字，用于编写在GPU上执行的设备端代码。CUDA库提供了丰富的API，帮助开发者高效地管理和操作GPU设备，完成复杂的并行计算任务。

CUDA提供了两种不同层次的API，分别适用于不同的开发需求：

- **CUDA 驱动 API**：低级别的 API，提供了对GPU设备的底层控制。使用驱动API可以直接操作GPU的硬件资源，灵活性高，但使用起来较为复杂，适合对性能有极致要求的开发者。
- **CUDA 运行时 API**：高级别的 API，封装了许多底层操作，使用起来更加简单和直观。运行时 API 基于驱动 API 实现，适合大多数开发者使用。运行时 API 会自动处理许多底层细节，如内存管理、线程调度等。

![](/images/Professional%20CUDA%20C%20Programming/Pasted%20image%2020250123212538.png)

需要注意的是，CUDA 驱动时 API 和运行时 API 是**互斥的**，开发者在一个项目中只能选择其中一种API，不能混合使用。选择哪种API取决于项目的需求和开发者的经验水平。

一个典型的 CUDA 应用程序通常由两部分组成：

- **CPU 主机端代码**：这部分代码运行在 CPU 上，负责控制整个程序的流程，包括数据初始化、内存分配、任务调度等。主机端代码通常使用标准的 C/C++ 编写，并由主机编译器（如 GCC 或 MSVC）编译。
- **GPU 设备端代码**：这部分代码运行在GPU上，负责执行实际的并行计算任务。设备端代码使用 CUDA C编写，并通过 CUDA 编译器 `nvcc` 进行编译。设备端代码通常以**核函数（Kernel Function）** 的形式存在，核函数是CUDA编程的核心部分，定义了在 GPU 上并行执行的任务。

CUDA 编译器 `nvcc` 会自动将主机端代码和设备端代码分离，并分别编译。如图中主机端代码由本地 C 编译器处理，而设备端代码则由 `nvcc` 编译为 GPU 可执行的二进制代码。链接阶段，`nvcc`会将主机端和设备端的代码合并，比如内核程序调用或者明显的GPU设备操作时添加运行时的库，生成最终的可执行文件。

**注意：核函数是我们后面主要接触的一段代码，就是设备上执行的程序段**

![](/images/Professional%20CUDA%20C%20Programming/Pasted%20image%2020250123212719.png)

nvcc 是从 LLVM 开源编译系统为基础开发的。

![](/images/Professional%20CUDA%20C%20Programming/Pasted%20image%2020250123212743.png)

CUDA 工具箱提供编译器，数学库，调试优化等工具，当然 CUDA 的文档是相当完善的，可以去查阅，当然在我们基本了解基础结构的情况下，直接上来看文档会变得机械化。

![](/images/Professional%20CUDA%20C%20Programming/Pasted%20image%2020250123212814.png)

### 3. CUDA Hello World

以下是一个经典的 CUDA "Hello World" 示例，展示了如何在GPU上启动并行线程并输出信息。代码位于文件夹 `chapter01/hello_world.cu` 中：

```C
#include "../common/common.h"
#include <stdio.h>

/*
 * A simple introduction to programming in CUDA. This program prints "Hello
 * World from GPU! from 10 CUDA threads running on the GPU.
 */

__global__ void hello_world_GPU(void) {
    printf("Hello World from GPU! Thread ID: %d\n", threadIdx.x);
}

int main(int argc,char **argv) {
  printf("Hello World from CPU!\n");
  
  hello_world_GPU<<<1,10>>>();
  CHECK(cudaDeviceReset());    //if no this line ,it can not output hello world from gpu
  return 0;
}
```

#### 3.1 关键代码解析 Key Code Analysis

1. **`__global__` 关键字**

-   **作用**：声明一个函数为**核函数（Kernel Function）**，表示该函数将在GPU上执行。
-   **特点**：
    -   核函数只能从主机（CPU）调用，且必须返回`void`。
    -   核函数内部可以访问GPU的全局内存和线程索引（如`threadIdx.x`）。

2. **核函数调用语法 `<<<grid, block>>>`**

-   **含义**：指定核函数的执行配置（Execution Configuration），定义线程的组织方式。
    -   **`grid`（网格）**：由多个线程块（Block）组成。此处 `<<<1, ...>>>` 表示仅启动1个线程块。
    -   **`block`（块）**：每个块包含多个线程。此处 `<<<..., 10>>>` 表示每个块有10个线程。
-   **线程索引**：在核函数中，`threadIdx.x` 表示当前线程在块内的索引（从0开始），可通过它区分不同线程的任务。

这句话C语言中没有 `<<<>>>` 是对设备进行配置的参数，也是 CUDA 扩展出来的部分。

3. **`cudaDeviceReset()` 的作用**

-   **隐式同步**：CUDA 默认是异步执行的，即 CPU 调用核函数后不会等待 GPU 完成，而是继续执行后续代码。若未同步，程序可能在 GPU 任务完成前终止，导致输出丢失。所以我们要**等 GPU 执行完了，再退出主机线程**。
-   **解决办法**：
    -   **显式同步**：使用 `cudaDeviceSynchronize()` 等待所有GPU任务完成。
    -   **隐式同步**：`cudaDeviceReset()` 会强制释放GPU资源并隐式同步，确保输出完整性。

#### 3.2 CUDA 程序的标准流程 Standard Flow of CUDA Programs

完整的CUDA程序通常包含以下步骤：

1. 分配 host 内存，并进行数据初始化
   
```C
float *h_data = (float*)malloc(N * sizeof(float));
```
   
2. 分配 device 内存，并从 host 将数据拷贝到 device上 ；

```C
float *d_data;
cudaMalloc(&d_data, N * sizeof(float));
cudaMemcpy(d_data, h_data, N * sizeof(float), cudaMemcpyHostToDevice);
```

3. 调用 CUDA 的核函数在 device 上完成指定的运算；

```C
kernel<<<grid, block>>>(d_data);
```

4. 将 device 上的运算结果拷贝到 host上 ；

```C
cudaMemcpy(h_data, d_data, N * sizeof(float), cudaMemcpyDeviceToHost);
```

5. 释放 device 和 host 上分配的内存。

```C
cudaFree(d_data);
free(h_data);
```

上面的 hello world 只到第三步，没有内存交换。

#### 3.3 实际应用中的注意事项  Practical Considerations in Applications

1. **错误检查**

-   **核函数调用错误**：CUDA核函数本身不返回错误码，需通过`cudaGetLastError()`捕获错误：

```C
hello_world_GPU<<<1, 10>>>();
CHECK(cudaGetLastError());  // 检查核函数启动是否成功
CHECK(cudaDeviceSynchronize());  // 显式同步并检查执行结果
```

2. **线程组织优化**

-   **块与线程的平衡**：每个块的线程数不宜超过GPU硬件限制（通常为1024）。例如，若需启动10000个线程，可配置为`<<<10, 1024>>>`。
-   **多维线程组织**：CUDA 支持三维线程块和网格，适用于图像处理等场景：
  
```C
dim3 grid(4, 4);    // 4x4网格
dim3 block(8, 8);   // 8x8线程块
kernel<<<grid, block>>>();
```

### 4. CUDA C 难么  IS CUDA C PROGRAMMING DIFFICULT

CUDA C 编程的难度主要取决于开发者对 **GPU架构** 和 **并行编程范式** 的理解深度。与传统的CPU编程相比，GPU编程需要开发者更关注硬件特性（如内存层次、线程调度）和并行任务的优化策略。  

#### 4.1 CPU与GPU编程的核心差异 Main Differences Between CPU and GPU Programming

| 特性         | CPU                          | GPU                              |  
| ------------ | ---------------------------- | -------------------------------- | 
| 核心设计目标 | 低延迟、复杂逻辑处理         | 高吞吐量、大规模数据并行         |     
| 线程模型     | 重量级线程，上下文切换开销大 | 轻量级线程，上下文切换近乎零开销 |     
| 内存层次     | 缓存层级少，依赖高主频       | 多级缓存，显存带宽高             |     
| 适用场景     | 分支逻辑复杂、任务串行性强   | 数据密集型、逻辑简单且可并行化   |     

例如，若需处理一个包含大量分支判断的任务（如递归算法），CPU 的强逻辑处理能力更优；而若需对百万像素进行相同的滤波操作，GPU 的并行计算能力则显著占优。

#### 4.2 数据局部性 Data Locality

数据局部性是提升 GPU 程序性能的关键，分为两类：

1.  **空间局部性（Spatial Locality）**
    -   **定义**：指在相对较接近的存储空间内数据元素被连续访问
    -   **优化**：
        -   **合并内存访问**：确保相邻线程访问相邻内存地址（如使用`float4`类型代替`float`）。
        -   **利用共享内存**：将全局内存中的数据块加载到共享内存中，减少全局内存访问次数。
            
2.  **时间局部性（Temporal Locality）**
    -   **定义**：在相对较短的时间段内数据和/或资源被多次访问
    -   **优化策略**：
        -   **寄存器重用**：尽量将频繁使用的变量存储在寄存器中。
        -   **缓存预取**：通过预加载数据到缓存，减少重复访问全局内存的开销。

例如，在矩阵乘法中，将子矩阵加载到共享内存，使得每个线程块重复使用局部数据，显著减少全局内存访问。

#### 4.3 CUDA 性能模型 CUDA Performance Model

CUDA中有两个模型是决定性能的：**内存层次结构**和**线程层次结构**

1. **内存层次结构**

CUDA内存分为多个层级，访问速度与作用范围不同：

-   **全局内存（Global Memory）**：容量最大，但延迟高（需优化合并访问）。
-   **共享内存（Shared Memory）**：块内线程共享，速度接近 L1 缓存。
-   **寄存器（Registers）**：速度最快，但数量有限（每个线程独立）。
-   **常量内存（Constant Memory）**：只读，适合存储频繁访问的常量数据。
-   **纹理内存（Texture Memory）**：针对图像处理优化，支持缓存和插值。

2. **线程层次结构**

CUDA线程组织为 **网格（Grid）→ 线程块（Block）→ 线程（Thread）**：

-   **线程块大小**：通常设置为32的倍数（如128、256），以匹配 GPU 的 Warp 调度机制（32线程为一组）。
-   **网格大小**：根据总线程数动态调整，避免线程块过多导致调度开销。

CUDA C 写核函数的时候我们只写一小段串行代码，但是这段代码被成千上万的线程执行，所有线程执行的代码都是相同的，CUDA 编程模型提供了一个层次化的组织线程，直接影响GPU上的执行顺序。

CUDA性能模型是我们后面要研究的，线程，内存是主要研究的对象，我们能用到的工具相当丰富，NVIDIA为我们提供了：

| 工具                  | 功能                     | 应用场景         |
|---------------------|------------------------|--------------|
| NVIDIA Nsight       | 集成调试、性能分析和代码优化         | 可视化分析核函数执行效率 |
| CUDA-GDB            | 命令行调试器，支持断点、变量监控       | 定位核函数中的逻辑错误  |
| CUDA-MEMCHECK       | 检测内存越界、未初始化访问等错误       | 调试内存相关的问题    |
| nvprof/nv-nsight-cu | 性能分析工具，统计内存带宽、计算吞吐量等指标 | 优化瓶颈代码       |
| Nsight Compute      | 细粒度分析核函数的指令吞吐、内存占用等    | 高级性能调优       |

### 5. 总结 Summary

本文从总体上粗略的介绍了 CUDA 这种高效的异构计算平台，并且概括了我们的将要遇到的苦难和使用到的工具，当我们学会了 CUDA，那么编写高效异构计算就会像我们写串行程序一样流畅。 

---

## 参考引用

### 书籍出处

- [CUDA C编程权威指南](../../../asset/CUDA%20&%20GPU%20Programming/CUDA%20C编程权威指南.pdf)
- [Professional CUDA C Programming](../../../asset/CUDA%20&%20GPU%20Programming/Professional%20CUDA%20C%20Programming.pdf)

### 网页链接

- [人工智能编程 | 谭升的博客](https://face2ai.com/program-blog/)
- [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)
- [CUDA C++ Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html)
- [CUDA编程入门极简教程](https://zhuanlan.zhihu.com/p/34587739)