

## LLM Inference


本文是论文《Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems》的大部分内容的中文翻译，翻译使用 DeepseekV3 和 GPT-4o-mini 模型。

### 1. 摘要

在人工智能（AI）快速发展的背景下，生成式大型语言模型（LLMs）处于前沿，彻底改变了我们与数据交互的方式。然而，部署这些模型所需的计算强度和内存消耗在服务效率方面带来了巨大挑战，尤其是在需要低延迟和高吞吐量的场景中。本综述从机器学习系统（MLSys）研究的角度出发，探讨了高效LLM服务方法的迫切需求，这一需求处于先进AI创新与实际系统优化的核心。我们提供了深入的分析，涵盖了一系列解决方案，从尖端的算法修改到系统设计的突破性变革。本综述旨在全面理解高效LLM服务的现状和未来方向，为研究人员和实践者提供宝贵的见解，以克服有效部署LLM的障碍，从而重塑AI的未来。

### 2. 简介

生成式大型语言模型（LLMs）已成为推动人工智能（AI）重大进步的动力，并在各种语言相关任务中展示了卓越的性能。从机器翻译到情感分析、问答系统和文本生成，这些模型在理解、生成和处理人类语言方面展现了其强大的能力。基于Transformer架构的模型，如GPT系列（生成预训练变换器）、LLaMA系列以及其他最新的公开LLM（例如OPT、BLOOM、Mistral、DeciLM、Baichuan、GLM）在这一范式转变中发挥了至关重要的作用，彻底改变了自然语言处理（NLP）任务的处理方式。除NLP外，这些模型还正在改变更广泛的应用领域，包括自动编程、科学发现、个性化数字助手、创意艺术和下一代计算架构，展现了其多样性和在各个行业中的深远影响。然而，LLMs前所未有的成功也带来了几个挑战，最显著的就是它们在服务过程中的巨大计算需求。模型的庞大规模和复杂性，加之对广泛计算资源的需求，阻碍了它们在现实应用中的广泛部署。这些模型资源密集型的特性引发了对能耗、可扩展性和可获取性的担忧，限制了没有丰富计算资源（如大型公司）社区的普遍采用。本文旨在解决高效LLM服务的关键需求，全面探讨研究社区提出的应对这一挑战的多元化现有策略。我们对整个解决方案范围进行了深入考察，涵盖从算法创新到新型系统架构的各个方面，旨在优化大型语言模型的推理过程。

### 3. 背景

#### 3.1. 基于Transformer的大型语言模型

基于Transformer的大型语言模型（LLMs）标志着自然语言处理领域的重大转变，为理解和生成人类语言引入了新的范式。这一创新的核心是Transformer架构，其建立在**自注意力机制**的基础上，使模型能够在进行预测时权衡输入数据不同部分的重要性。从数学角度来看，Transformer中的自注意力机制可以描述如下：

对于输入序列 𝑋 = [𝑥₁, 𝑥₂, ..., 𝑥ₙ]，Transformer通过对𝑋进行线性变换计算出一组查询𝑄、键𝐾和值𝑉。自注意力得分则通过以下公式计算：


$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

其中，𝑑ₖ是键的维度。这一机制使模型**能够为输出的每个元素聚焦于输入序列的不同部分**，从而捕捉复杂的依赖关系，无论它们在输入序列中的距离如何。

Transformer中的另一个重要结构是**前馈神经网络（FFN）**，它存在于Transformer的每一层中，并显著增加了其计算强度。FFN通常由**两个线性变换**组成，中间夹着一个非线性激活函数，通常表示为：

$$
\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
$$

其中，𝑊₁、𝑊₂、𝑏₁和𝑏₂是FFN的可学习参数，非线性函数max(0, ·)（此处为ReLU）为模型引入了必要的非线性，**使其能够学习更复杂的模式**。FFN占据了模型参数数量的很大一部分，因此也显著影响了其内存占用和计算负载。

在每一层Transformer中，**多头注意力机制（MHA）** 从输入的不同部分聚合信息后，FFN会独立处理每个位置的聚合信息。这种并行处理能力是Transformer的一个关键优势，使其能够有效处理序列。然而，这也意味着计算负载和内存需求会随着输入序列的长度和网络的深度而增加。

基于Transformer的LLMs通过结合自注意力和FFN，使这些模型能够捕捉广泛的上下文和语言细微差别，从而在各种NLP任务中树立了新的基准。然而，训练和推理过程中巨大的计算需求已成为一个关键研究领域，重点在于在不显著影响性能的情况下优化这些方面。

Transformer模型还包括其他关键组件，例如**位置编码**（为序列中每个标记的位置信息进行编码）以及**多头注意力机制**（使模型能够在不同的表示空间中聚焦于序列的不同部分）。

#### 3.2 GPU与其他加速器

LLMs的快速发展在很大程度上得益于GPU架构及其他加速器的演进，这些硬件在提升模型性能和效率方面发挥了重要作用。GPU（图形处理单元）已成为这一领域的基石，主要归功于其卓越的并行处理能力。与设计用于顺序处理的传统CPU不同，GPU由数千个小型高效核心组成，能够同时处理多个任务。这使得它们特别适合深度学习计算中无处不在的**矩阵和向量运算**，尤其是基于Transformer的模型。

典型的GPU架构由一系列**流式多处理器（SMs）** 组成，每个SM包含多个核心，这些核心共享一个指令单元，但可以并行执行独立的线程。此外，每个SM内的**共享内存**（SRAM）允许线程之间**高效地交换数据和同步**，从而显著优化了LLM计算中所需的内存访问模式。这种设计对LLM中的计算密集型任务（如Transformer中的自注意力和前馈网络计算）尤为有益。

GPU还配备了**高带宽内存**（HBM），能够实现更快的数据传输速率，显著**减少大规模计算中与内存访问**相关的瓶颈。此外，最新的GPU架构（如NVIDIA的Ampere和Hopper架构）不断提供增强功能，推动LLM计算的边界，例如改进的内存带宽和容量、更高的每秒浮点运算次数（FLOPS）、专门的混合精度计算单元（如Tensor Core）以及更高效的资源利用率，进一步加速了LLM的性能。其中一些架构支持多种精度格式，包括FP32（32位浮点）、TF32（TensorFloat-32）、FP16（16位浮点）、**BF16**（Brain Floating Point）甚至**INT8/INT4**，允许在计算速度和数值精度之间进行灵活权衡，这对优化LLM性能至关重要。

除了GPU之外，研究人员还探索了多种硬件平台用于LLM部署，包括CPU、移动和边缘设备、ASIC以及专用加速器（如TPU、FPGA）和其他厂商推出的新兴AI芯片（例如Apple M2 Ultra、AWS Inferentia、SambaNova、Cerebras、Graphcore IPU）。本综述主要强调基于GPU的研究，这一重点源于多方面的技术动机。由于其架构创新和卓越的计算能力，GPU在过去几年中主导了大规模深度学习的研究领域。此外，GPU的编程语言（如NVIDIA的CUDA和AMD的ROCm）提供了对**线程层次结构的细粒度控制**，使研究人员能够充分利用GPU固有的高度并行性。这吸引了大量开发者在这些GPU上构建成熟的软件生态系统，推动了大多数开创性和先进的LLM研究。

尽管其他硬件平台在特定场景中确实具有独特优势，但围绕GPU展开的大量研究、开发和部署使其成为深入理解LLM推理方法不可或缺的参考。考虑到硬件相似性，其他硬件平台也可以从本综述讨论的设计理念、见解和方法中受益。

#### 3.3 LLM推理

LLM推理，尤其是在GPT（生成式预训练Transformer）等模型中，通常采用**自回归解码**方法。这种方法对于这些模型生成文本的方式至关重要，确保生成的每个新词或标记都考虑到迄今为止生成的整个序列。自回归解码的工作原理是，**在给定所有先前标记的情况下，依次预测序列中的下一个标记**，如算法1所示。

**算法1：LLM推理的自回归解码**

$$
\begin{aligned}
&\text{使用给定的上下文或起始标记初始化输入序列 } X_0 \\
&\text{for } t = 1 \text{ to } T \text{ do} \\
&\quad \text{预测下一个标记 } y_t = \arg\max_y P(y|X_{t-1}) \\
&\quad \text{更新输入序列 } X_t = X_{t-1} \oplus y_t \\
&\quad \text{if } y_t \text{ 是EOS（结束标记） then} \\
&\quad \quad \text{break} \\
&\text{end for}
\end{aligned}
$$

在这里，𝑃(𝑦|𝑋𝑡−1) 表示在给定当前序列𝑋𝑡−1的情况下，下一个标记𝑦的**概率**，⊕表示连接操作。argmax函数用于**在每一步选择最可能的下一个标记**。这种自回归方法在LLM推理中对于生成连贯且上下文合适的文本至关重要。它**确保生成的每个标记都基于对所有先前生成内容的全面理解**，从而使LLM能够生成高度相关且流畅的文本序列。先前的研究对基于Transformer的LLM推理的算法强度进行了深入分析（例如，计算FLOPS、I/O和内存消耗），并根据自回归解码算法的执行提供了广泛的成本估算实证结果（例如，建模推理延迟）。LLM推理的优化是一个复杂的问题，因为不同的算法配置和系统设置可能存在不同的最优策略。

#### 3.4 挑战

本节描述了高效LLM服务所面临的各种挑战。

- **延迟与响应时间**：高效的大型语言模型推理需要实现低延迟和快速响应时间，尤其是在**聊天机器人、虚拟助手和交互式系统**等实时应用中。在模型复杂性与推理速度之间取得平衡是一个关键挑战，这需要优化算法和系统架构，以在不影响准确性的前提下最小化响应时间。

- **内存占用与模型大小**：由于大型语言模型的规模庞大且参数数量众多，它们对内存的需求非常高。在**内存受限的设备上部署**此类模型是一个挑战，需要开发有效的模型压缩技术和系统优化方法，以在不牺牲性能的情况下减少内存占用。

- **可扩展性与吞吐量**：推理系统在生产环境中通常面临不同程度的请求负载。确保可扩展性和高吞吐量以高效处理多个并发请求，需要并行计算、请求调度和其他系统级优化，以有效分配计算资源。

- **硬件兼容性与加速**：高效利用硬件资源对于大型语言模型推理至关重要。将LLM模型适配到包括**CPU、GPU和专用加速器**在内的多样化硬件平台和架构，需要硬件感知的算法设计和优化，以充分发挥底层硬件的潜力。

- **准确性与效率之间的权衡**：优化LLM推理的效率有时可能涉及与模型准确性之间的权衡。**在模型大小、计算复杂性和性能之间找到合适的平衡点**是一项具有挑战性的任务，需要仔细考虑和评估各种算法和系统级技术。

### 4 分类

现有关于提升LLM服务效率的努力可以大致分为两类，包括**算法创新**和**系统优化**，以下将分别进行讨论。

![](../../../../images/LLM/Pasted%20image%2020250227112134.png)

#### 4.1 算法创新

本节全面分析了为优化语言模型推理效率而提出的各种算法和技术。这些工作旨在通过算法改进解决大规模Transformer模型的固有性能缺陷。

##### 4.1.1 解码算法

在本节中，我们回顾了如图2所示的新型解码算法，这些算法优化了LLM的推理过程。这些算法旨在减少计算复杂度，并提升生成任务中语言模型推理的整体效率。

![](../../../../images/LLM/Pasted%20image%2020250227103028.png)


- **非自回归解码**

现有LLM的一个主要局限是**默认自回归解码机制，逐顺序生成输出标记**。为了解决这一问题，一种代表性的方法是**放弃自回归生成范式，并行解码输出标记**。**非自回归解码**最初被提出用于**加速机器翻译**，通过**在解码过程中打破词依赖关系并假设一定程度的条件独立性**。为了缓解翻译质量下降的问题，一些后续研究如**半自回归解码**进一步扩展了这些非自回归方法，通**过建模输出依赖关系或迭代优化输出标记**来达到自回归模型的质量。**块级并行解码**在基础LLM中插入一个前馈层，以并行预测多个未来位置，然后回退到基础模型验证的最长前缀。然而，这些方法需要以新的依赖关系重新训练一个新的LLM或调整原始LLM的部分层，这并不总是可行的。最近的一些努力致力于在一个解码步骤中生成多个标记，而无需对模型进行任何训练或修改。**并行解码**将贪婪自回归解码重新定义为可并行求解的非线性方程组，利用雅可比和高斯-赛德尔定点迭代方法实现快速推理。关于非自回归翻译的全面综述已被提出，总结了这一方向的最新进展。到目前为止，**由于对输出标记之间条件依赖关系的忽视，大多数非自回归方法的输出质量仍然不如自回归方法可靠**，尽管解码速度有所提升。

- **推测解码**

另一类工作通过利用**推测执行**来提高解码并行性，从而解决顺序执行的限制。**自回归LLM推理过程中的每个解码步骤都可以被视为带有条件分支的程序执行，例如决定下一个生成哪个标记**。推测解码被提出，以高效的方式（例如使用参数较少的草稿模型）首先**对多个步骤进行解码预测**，并同时使用LLM验证这些预测。然而，将推测解码应用于LLM时仍存在一些实际挑战，例如如何使解码预测足够轻量且准确，以及如何利用LLM实现高效的并行验证。SpecInfer 首次通过引入多个小型草稿模型并结合一种新颖的**基于树的推测推理和标记验证机制**解决了这些挑战，并提出了一种低延迟的LLM服务系统实现。推测解码的主要优势在于它在**不改变输出的情况下提高了并行性**。这种保证来自于预测输出始终由原始LLM验证，并且在预测出错时回退机制会生效。

- **早期退出**

其他一些研究尝试利用现有LLM的深层多层级架构，并通过早期退出机制加速解码过程。其直觉是，**早期模型层的输出有可能自信地推断目标分布**。它们可以基于内部分类器发出预测，而无需运行整个LLM，并且已经探索了各种**退出条件**。这些方法也被称为自适应计算，因为它们根据请求调整计算量以分摊总推理成本，即对较简单的推理请求使用更少的计算。总体而言，这些方法大多受限于内部表示携带的信息不足，可能无法忠实做出准确预测。

- **级联推理**

受推理请求复杂性的变化驱动，级联推理**采用一系列不同规模的LLM以最小化响应时间**。CascadeBERT 并未直接为每个查询使用大规模模型，而是涉及一系列对应于不同模型深度的内部分类器，以级联方式组织它们，并根据实例难度自适应选择适当的分类器。Tabi 针对判别模型（即非生成式LLM）进行了优化，但它采用了类似的方法，结合小型模型和LLM来处理不同置信度的查询。FrugalGPT 利用基于学习的方法自适应地将查询分配给不同的LLM API，优化成本和性能。一项并行工作联合优化了模型复用和查询缓存，并分析了最小化推理成本的最优性。Mixture-of-thought 将级联思想扩展到LLM推理任务以节省成本，从Chain-of-Thought 和Program-of-Thought 提示中采样答案。总体而言，级联推理是提升推理效率的一个有前景的方向，但设计一个准确的调度机制以避免影响模型质量仍然具有挑战性。

##### 4.1.2 架构设计

本小节探讨了为大型语言模型量身定制的创新架构设计。研究人员提出了超越原始Transformer的新颖模型架构，在模型大小、性能和效率之间取得平衡，为更快且资源高效的推理开辟了新途径。

- **配置缩减**

为了降低LLM推理的计算成本，一种直接的方法是缩减模型配置，例如使用**浅层编码器**或**解码器**、**权重共享**和**词汇表缩减**。然而，减少模型参数的数量也会影响下游任务的性能。

- **注意力简化**

自注意力计算的一个突出挑战是其**计算复杂度为$O(𝐿^2)$**，与输入序列长度𝐿呈二次方增长。许多Transformer变体被提出，**将标准注意力简化为更高效的替代方案，以应对超长序列任务**，例如**稀疏化、核化和因式分解**。最近，有一种趋势是借鉴先前的注意力简化方法，对其进行泛化和组合，以**缩短上下文并减少KV缓存的大小以及注意力复杂度**，同时略微降低解码质量（例如，**滑动窗口注意力、基于哈希的注意力、扩张注意力**）。其中一类方法是通过**将上下文压缩为更少的软标记**（例如，用**摘要标记**或**地标标记**替换，利用**额外的自编码器方案**）或直接删除或重述不重要的上下文标记，基于不同的重要性指导（或称为**语义压缩**）。例如，**自适应稀疏注意力**采用基于学习的方法，动态消除每个标记的无信息上下文标记。Scissorhands 和 H2O 选择一些可能对未来解码过程有重大影响的重要标记，并保存它们的KV缓存。StreamingLLM 重视初始标记，并通过滑动窗口维护它们，这也与先前的工作类似。FastGen 允许不同的注意力头自适应地采用不同的强调模式。表1展示了四类代表性方法的稀疏注意力模式及其应用。然而，由于上下文不完整，这些方法在实际工作负载中可能面临不可避免的信息丢失，尤其是在注意力分布更复杂的情况下。
  
![](../../../../images/LLM/Pasted%20image%2020250227104725.png)

- **激活共享**

另一个方向是共享中间激活以提高注意力计算效率。**注意力共享方法**观察到不同层注意力矩阵分布的相似性，并重用这些注意力矩阵以减少计算成本。多查询注意力（MQA）使不同的头共享一组键和值，以减少增量推理中的内存带宽需求。组查询注意力（GQA）放宽了单组键和值的限制，允许多组键和值，每组与一组查询相关联。它们已被多个最近的公开LLM成功采用，并展示了其卓越性能，包括基于MQA的模型如Falcon、PaLM、ChatGLM2-6B以及**基于GQA的模型如LLaMA-2和Mistral-7B**。

- **条件计算**

**稀疏激活的专家混合（MoE）** 范式将模型的能力分配到多个“专家”中，这些专家是较小的神经网络，每个专家专门处理数据的不同子集。它允许系统基于某些路由机制仅为给定输入调用必要的专家，而不是在整个大规模模型上进行计算，从而实现计算和内存效率。例如，TaskMoE 表明，与标记级路由相比，任务级路由能够增加模型容量，同时提高推理吞吐量。随着LLM的不断增长，MoE架构成为确保未来LLM可扩展性和效率的一个有前景的方向。与此同时，MoE的动态特性也要求从**分布式通信**和**GPU内核实现**两方面进行特殊的系统优化，以提高MoE推理效率。

- **循环单元**

尽管**循环神经网络（RNN）**（例如LSTM）在**捕捉序列中的长期依赖关系方面往往存在困难**，但仍有一些方法使用循环单元替换Transformer模块，并在推理过程中实现线性的计算和内存复杂度，例如**RWKV**和RetNet。具体而言，与先前的方法不同，这些最近的探索大多基于线性注意力（即**线性Transformer**、无注意力Transformer）表示。经过改造后，它们通过使用线性循环单元（例如**状态空间模型**、**LRU**）建模标记之间的交互，克服了注意力的$O(𝐿^2)$瓶颈，这些单元更容易保持可并行训练的特性。它们的设计还包括各种**位置编码模块**、**指数衰减机制**以及一系列逐标记的**非线性MLP**或**GLU**，以提高模型表示能力。最近，它们在模型性能和计算效率方面都展示了有希望的结果。然而，循环单元是否能够成功替代Transformer用于LLM仍然是一个开放的问题（尤其是对于长序列）。

##### 4.1.3 模型压缩

本节深入探讨模型压缩技术，旨在通过创建更高效、更紧凑的模型来减少LLM的内存占用和计算需求，同时不显著降低性能。

- **知识蒸馏**

一种方法是知识蒸馏，即**通过大型教师模型的监督训练一个小型学生模型**。大多数先前的研究方向是探索**白盒蒸馏（white-box distillation）**，这需要访问整个教师模型的参数。由于基于API的LLM服务（如ChatGPT）的兴起，一些**黑盒蒸馏模型**引起了广泛关注，例如Alpaca、Vicuna、WizardLM等。这些模型通常具有更少的参数，但在各种下游任务中表现出与原始LLM（如GPT-4）相当的性能。

- **网络剪枝**

**网络剪枝**方法在过去几年中得到了广泛研究，但并非所有方法都直接适用于LLM。必须考虑到重新训练可能带来的高昂计算成本，并评估剪枝是否基于底层系统的实现显著提高了推理效率。最近的一些方法在LLM上应用**结构化剪枝**方法，**移除整个结构化LLM组件**，从而促进GPU加速。例如，Deja Vu根据上下文稀疏性假设剪枝特定的注意力头和MLP参数，而无需修改预训练模型。在**非结构化方法**方面也有一些最新进展，通常可以实现50-60%的稀疏度用于LLM压缩。值得注意的是，它们可以进一步推广到半结构化N:M稀疏度（即2:4和4:8），在NVIDIA稀疏张量核心的加速下显著提高推理速度。LoSparse 和 DSFormer 通过**低秩分解**用一个小型密集矩阵和一个稀疏半结构化矩阵近似模型权重。Flash-LLM 通过为使用张量核心的非结构化剪枝提供内存高效的SpMM实现，放宽了这一要求。PowerInfer 假设这些稀疏激活神经元的访问具有偏斜性，并提出了一种GPU-CPU混合推理引擎，使GPU和CPU处理不同的神经元。

#### 4.2 系统优化

本节探讨了在不修改LLM计算语义的情况下加速LLM推理的系统优化技术。这类工作的目标是通过优化用于大型语言模型推理的底层系统和框架来提高系统效率。

##### 4.2.1 低比特量化

本节探讨了最先进的低比特量化技术，这些技术能够高效地表示模型权重和激活值。通过**使用更少的比特（即少于32位）来表示数值**，这些方法显著**减少内存消耗**，并在硬件平台上加速了推理。

一种方法是对LLM进行量化，这些量化方法可以简要分为两个方向：**量化感知训练**（QAT）和**训练后量化**（PTQ）。PTQ通过使用自定义CUDA内核或编译将**模型权重甚至激活值的计算精度降低为INT8或INT4**，以实现效率提升，例如W8A16（即仅对权重进行INT8量化，激活值保持FP16或BF16）、GPTQ 中的W4A16、SmoothQuant 中的W8A8以及W4A4。硬件的演进也满足了这些需求。一个支持证据是，NVIDIA最近的架构（如Turing和Ampere）已经包含了INT8和INT4张量核心，而最新的Hopper架构虽然取消了INT4支持，但引入了FP8张量核心以提高数值精度（例如，H100 GPU在FP8下的性能可达FP32的60倍）。现有方法通常采用各种量化函数，包括**均匀方法**（即四舍五入）和**非均匀方法**。为了缓解低精度带来的性能损失，QAT**在模型训练过程中集成了量化**。值得注意的是，由于底层系统实现的挑战，低精度量化方法可能会导致推理速度比传统精度（如FP16）更慢。虽然低精度方法显著减少了模型部署的资源需求，但也有研究表明，**由于缩放定律的存在，量化方法可能会对模型的推理性能产生显著影响**。此外，量化还被应用于**上下文压缩**（例如CacheGen）和**内存高效微调**（例如QLoRA、PEQA），从而降低了LLM推理的内存消耗。

##### 4.2.2 并行计算

本节探讨了针对大型语言模型量身定制的并行计算策略。这些方法利用现代硬件架构的并行处理能力，**将计算分布到多个核心或设备上**，从而在推理过程中实现显著的加速。

- **模型并行**

大多数模型并行方法最初是为**大规模深度神经网络（尤其是基于Transformer的模型）的分布式训练**提出的。例如，**张量模型并行**（TP）**将模型层（如注意力层、前馈网络）从内部维度（如头数、隐藏层维度）分割成多个部分，并将每个部分部署在单独的设备（如GPU）上**。通过并行计算，TP可以显著减少推理延迟，广泛用于同一机器内的多个GPU，尤其是在具有高速NVLink连接的场景中。**PaLM推理**通过引入2D张量并行扩展了TP在大规模Transformer推理中的应用，并声称在大型集群（超过256个设备）中具有较低的理论通信复杂度。对于仅使用一个头处理键和值的多查询注意力机制，它进一步将数据并行引入到混合张量分区策略中。**流水线模型并行**（PP）**将模型层按顺序排列在多个设备上，每个设备负责一个由多个连续模型层组成的流水线阶段**。虽然PP可以显著提高单位时间内处理的输入数量（吞吐量），但它并不像TP那样从本质上减少处理单个输入从头到尾所需的时间（延迟）。**序列并行**（SP）有各种不同的设计和实现，但其在LLM推理中的核心思想是**通过沿序列长度维度将长序列的处理分布到多个GPU上来分担计算和存储负载**。不同的并行技术引入了不同程度的通信开销和计算延迟。为了实现最佳性能和资源利用率，**自动并行**技术已被广泛研究，用于分布式训练（如Alpa、FlexFlow、Galvatron）。通过替换其成本模型以适应Transformer模型自回归推理的可预测运行时，可以轻松将先前的自动搜索算法（如动态规划、整数线性规划）应用于LLM服务（如AlpaServe、FlexFlow-Serve、SpotServe），并在无需人工干预的情况下确定最有效的并行策略。还有一些方法支持卸载技术，**利用更大但更慢的内存（如CPU DRAM）来保存模型参数和KV缓存**，以补充有限的设备内存（如GPU DRAM）。

- **去中心化推理**

这类方法**结合了模型并行和数据并行**，多个去中心化的自愿节点协作处理数据并推断输出。这种方法**在硬件资源地理分布的场景**中尤为有用。受众包计算的启发，Petals通过互联网上的协作消费级GPU为BLOOM-176B模型提供服务。去中心化推理为利用被忽视的消费级GPU运行LLM开辟了新方向，但也面临一些实际挑战，例如**设备异构性、计算和内存容量有限、低带宽网络、容错性和隐私保护**。

##### 4.2.3 内存管理

高效的内存管理在LLM服务中始终是一个突出挑战，尤其是考虑到Transformer架构固有的**内存密集特性**。随着对长序列推理需求的增加，**KV缓存的内存占用**成为了优化的重点目标，相较于模型权重和其他激活所需的工作空间。由于KV缓存的内存在增量解码过程中动态且不可预测地增长和缩小，简单的方法（**例如FasterTransformer**）会**预先分配一个连续的内存块，以最大序列长度为假设**。这对于1）请求长度各异的输入批次和2）在并行生成多个输出序列（例如，束搜索、并行解码）的复杂解码场景而言，会**严重浪费内存**。**vLLM提出了分页注意力**，它**将KV缓存划分为不连续的内存块，从而大幅提高批量大小和吞吐量**。SpecInfer 提出了树状注意力和深度优先树遍历，以消除为多个共享相同前缀的输出序列分配冗余的KV缓存。LightLLM则采用了更加细粒度的按_token_级别的内存管理机制，以进一步降低内存使用。然而，此类**碎片化内存管理机制**的开销带来了新的挑战，尤其是在采用其他优化以增加批量大小的情况下，这些细粒度的内存管理方法可能仅提供微弱的吞吐量收益，同时显著放大推理延迟。显而易见，在LLM推理中的内存减少与其他算法创新和系统级优化密切相关。尽管某些方法可能适用于特定工作负载，但它们可能互相抵消，导致整体性能下降。找到LLM推理系统在内存效率与计算性能之间的适当平衡，是该领域一个尚待解决的重要挑战。

##### 4.2.4 请求调度

高效调度即将到来的推理请求对于优化LLM服务至关重要。本节回顾了请求调度算法，这些算法旨在最大化资源利用率，确保响应时间在延迟服务水平目标（SLO）内，并有效处理不同的请求负载。LLM服务的请求调度与一般机器学习服务技术有许多共同之处，因为两者都旨在高效管理到来的请求并优化资源利用。这些共同点包括**动态批处理**、**抢占**、**优先级**、**交换**、**模型选择**、**成本效率**、**负载均衡和资源分配**。然而，由于LLM的独特特性，如**模型规模巨大、迭代自回归解码机制、未知的可变输出长度以及上下文信息的状态管理**，LLM服务还面临独特的挑战。早期的LLM服务系统（例如在NVIDIA Triton上的FasterTransformer）仅支持请求级调度，类似于以前的方法。Orca 首次注意到生成LLM与之前机器学习推理系统的请求级调度之间的差距。考虑到可变的输出序列长度，它以先到先服务（FCFS）的顺序调度引擎的执行，按迭代粒度进行调度，并为更好的硬件利用率批处理选定的操作。一系列后续方法继承了选择性批处理和迭代级调度策略，例如在vLLM和RayLLM中的持续批处理，以及在TensorRT-LLM中的**实时批处理**。此外，SpecInfer进一步扩展到推测解码，通过迭代选择一批请求以执行一次推测推理和验证。FastServe关注作业完成时间（JCT），并采用迭代级抢占，对输入长度较短的请求优先考虑，而不是使用FCFS。SARATHI旨在解决因初始迭代的可变长度输入请求而造成的分布式推理中的管道气泡。为了充分利用GPU计算，它将输入提示拆分为均匀的块，并在可能的情况下将该块插入到其他请求的解码迭代中，这一策略还被DeepSpeed-FastGen称为**动态拆分融合**（Dynamic SplitFuse）采用。S3涉及输出序列长度预测器，帮助在GPU内存限制内安排更多的并发请求，以实现更大的批量大小和更高的推理吞吐量。

##### 4.2.5 内核优化

本节深入探讨内核级优化，这些优化针对语言模型推理管道中特定操作的性能。这些优化利用硬件特定功能和软件技术来加速关键计算内核。

- **内核融合**

为了**减少内核启动和内存访问的开销**，内核融合被先前的深度神经网络框架和编译器广泛采用。由于**LLM推理不需要反向计算**，因此存在更多的内核融合机会。一些当代Transformer推理引擎（如FasterTransformer、TenTrans、TurboTransformers、LightSeq、ByteTransformer）和编译器（如Welder）提出融合以下操作：1）**形状相同的矩阵乘法（GEMM）**（例如查询、键和值的三个线性变换）；2）**将偏置加法与其他非GEMM内核（如残差连接、层归一化和激活函数（如ReLU））融合**。其中，融合多头注意力内核的优化已被广泛探索，并将在下文讨论。

- **定制注意力**

为了使注意力操作在GPU上高效运行，**专门为注意力计算定制GPU内核**至关重要。例如，**cuDNN已经提供了一个融合多头注意力内核的API**。同时，一些开源实现为了更高的性能增益也被提出。由于特殊的自回归解码机制，这些实现大致可以分为两类。**一类是针对第一次迭代（即初始/预填充/上下文/提示阶段），并行处理输入提示中的所有令牌**。例如，xFormers使用CUTLASS将**在线softmax技巧扩展到整个注意力计算中**。另一类是**针对后续迭代（即增量/解码/生成阶段）**，内核每次迭代只生成一个输出令牌。对于自回归解码，常见的做法是保存先前计算的键和值，以便在生成新令牌时只需计算单个查询，而无需重新运行整个序列。该领域优化的主要方向是**最大化线程占用率并最小化设备上高带宽内存（HBM）的访问（即使用共享内存或寄存器）**。它们通常通过批处理大小和头数维度（如FasterTransformer）并行化以分配工作负载。一些方法进一步通过**将KV缓存分割成块来并行化序列长度维度**，但需要在最后减少块级结果，例如FlashDecoding。后续工作FlashDecoding++通过引入预先已知的统一最大值，消除了部分softmax的同步。根据工作负载选择合适的并行维度以实现更好的线程利用率是必要的。

- **采样优化**

采样算法的选择会极大地影响LLM生成质量。默认的贪婪采样总是选择概率最高的令牌。**并行采样技术**（如束搜索）通过每次迭代维护固定数量（即束宽）的得分最高的序列来高效解码近似最优序列。各种随机采样技术（如top-𝑘、top-𝑝、温度控制）被提出以引入随机性，从而生成更多样化的输出。然而，它们仍然面临一些实际系统挑战。一是**冗余KV缓存带来的内存压力增加**，另一个是**由于LLM的大词汇量（即数万个）导致的采样效率问题**。例如，LightSeq提供了一种高效的分层实现，将词汇表分为𝑘组，使用少量GPU指令检索每组中的候选词，然后重新排序这些候选词以获得top-𝑘令牌。

- **可变序列长度**

LLM推理的另一个独特挑战是**序列的输入长度和输出长度都可能变化，且后者是未知的**。加速推理的一种方法是**同时处理批处理中的多个序列**。然而，当一批序列的输入长度不同时，通常使用填充使它们长度相同以进行批处理，这会浪费计算和内存资源。为了缓解这些低效问题，可以采用各种策略。**打包技术**将序列存储在连续的内存空间中，无需填充，仅在注意力计算前解包。**不规则张量**进一步支持使用编译器生成的内核进行最小填充的计算。将序列分桶为更小的计算粒度（如块）也是缓解填充令牌内存使用的一种可能解决方案。由于初始阶段和增量阶段的混合执行，**分桶输入提示**也给内存管理和请求调度带来了新的挑战。

- **自动编译**

大多数现有的LLM推理系统**利用供应商特定的库作为其后端**，如cuBLAS、cuDNN和CUTLASS，这些库提供了优化的内核实现。为了进一步提高推理效率，它们还大力优化了针对NVIDIA GPU的特定LLM操作符（如注意力）的手写内核。尽管有这些工作，使用自动化DNN编译器的趋势仍然存在，如**TVM**（即Unity、Relax和TensorIR）、**MLIR**、JAX、OpenAI **Triton**、TASO和TorchInductor。编译方法可以帮助发现潜在更高效的操作符实现（如表达式推导），更重要的是，有助于适应**替代硬件平台**，包括移动和边缘设备、CPU、深度学习加速器和其他类型的GPU（如AMD GPU和Apple M2 Ultra）。

### 5. 软件框架

生成式大语言模型（LLM）服务需要全方位的优化，近年来许多研究开始开发软件框架，以提供高效的LLM推理部署服务。接下来，我们将回顾这些系统，并对表2中几个代表性的基于GPU的开源LLM服务系统进行全面分析。该分析未包含一些流行的相关项目，包括：1）**针对其他硬件的专门解决方案**（如PopTransformer、CTranslate2、lammap.cpp和ggml）；2）基于其他系统构建的部署解决方案，如OpenLLM（vLLM）、xinference（ggml + vLLM + xFormers）、LMDeploy（FasterTransformer）、gpt-fast（PyTorch）、DeepSpeed-MII和DeepSpeed-FastGen（DeepSpeed-Inference），以及RayLLM和RayServe（vLLM）。我们比较了这些最先进的LLM服务系统，并从多个方面总结了它们的差异。

![](../../../../images/LLM/Pasted%20image%2020250227113353.png)

首先，**大多数系统支持张量并行**，以实现多GPU推理并提升系统性能。其中一些系统还支持**流水线并行或卸载**，以**分别支持多节点或资源受限环境下的推理**。其次，部分系统借鉴了Orca的思想，实现了**迭代级调度**。第三，我们研究了这些系统的注意力核，并分别介绍了它们在初始阶段和增量阶段的实现。在初始阶段，它们通常采用批**量通用矩阵乘法**（GEMM）方法（如cuBLAS、torch、Relay），部分系统还利用**在线softmax技巧**减少高带宽内存（HBM）访问（如Flash-attention、xFormers）。增量阶段更具挑战性，因为逐令牌生成方案导致计算强度较低。为了提高GPU利用率，FasterTransformer手动**将注意力计算（如线性投影、位置偏置、点积、softmax等）融合到一个高性能核模板**中，并采用了多种核优化技术，**如共享内存缓存**、**用于归约的warp-shuffle指令**、支持张量核心和多精度的**半矩阵乘法与累加（HMMA）**。FlexFlow-Serve支持推测解码，并提供了一个基于树的并行解码核，以零内存冗余和最大线程并行度验证来自多个序列（即来自多个小模型或不同波束或并行采样）的推测令牌。vLLM通过**将键值（KV）缓存分页**，扩展了FasterTransformer的融合多头注意力（MHA）核，以消除冗余内存使用，特别是在并行采样场景中。LightLLM则采取了后续方法，将KV缓存划分为更细粒度的令牌级片段。

需要注意的是，上述讨论并未涵盖所有重要方面。例如，即使是最流行的Flash和Paged注意力核，它们在这些系统中的实现方式也各不相同。TGI直接导入原始的Flash/Paged注意力库，LightLLM采用OpenAI Triton实现的核，MLC-LLM通过TVM生成核，而TensorRT-LLM则基于FasterTransformer的融合注意力核进行修改以支持分页注意力。另一个例子是关于输入感知的核选择。在初始阶段，TensorRT-LLM根据上下文长度从cuBLAS和Flash注意力中进行选择。除了注意力计算外，对于线性投影算子，最近也有一种趋势是用通用矩阵向量积（GEMV）替代GEMM，以更高效地处理小批量情况。这些系统还有许多其他不同的特性，如编程语言（即C++、Python）、低精度支持（即FP16、INT8）、支持的硬件和模型。

总的来说，这些设计和实现的不同选择很大程度上取决于它们的优先优化目标。例如，vLLM提出了分页注意力以提高批量大小，从而实现更高的吞吐量（𝑇𝑝𝑡），而FlexFlow-Serve则利用SpecInfer加速解码以降低延迟（𝐿𝑎𝑡）。基本上，**低延迟和高吞吐量是LLM服务系统中的双重优化目标**，代表了互补但常常冲突的目标，因此需要一种平衡策略来优化单个任务的快速响应与在指定时间范围内处理的最大任务量之间的权衡。最近的一些研究进一步将响应延迟分解为TTFT+TPOT×输出序列长度，其中TTFT表示首令牌时间，TPOT表示每输出令牌时间。前者由初始阶段处理速度驱动，而后者直接取决于增量解码期间的每次迭代执行时间。区分这两个指标对LLM服务提供商有益，从而引导不同的系统设计选择和用户体验（如更快的应用响应速度、更长的提示）。此外，降低货币成本也是某些LLM服务系统设计和实现的一个重要且实际的目标。尽管不太可能有一种放之四海而皆准的解决方案，但我们相信未来的LLM服务系统将继续整合这些差异化特性，从而持续提升系统效率和硬件利用率。

### 6. 基准测试

建立一个全面且可重复的基准测试，以比较各种LLM服务系统的性能，例如MLPerf，对学术界和工业界在这个领域都是一项重要的工作。这不仅有助于LLM用户选择合适的系统解决方案，还鼓励研究人员和开发者跟上先进的优化步伐。不幸的是，尽管之前有一些报告，截至目前，**社区尚未推出一个足够有说服力的基准，考虑到了所有影响因素**。这主要是由于众多评估设置，包括模型配置、硬件环境和请求负载等。在有限数量的设置组合下进行测试无法得出可信的结论。例如，某些系统优化技术只有在高负载或低负载条件下才能实现性能优势，反之，它们甚至可能是有害的。此外，在测量推理延迟时，由于系统设计的差异，如何排除与GPU推理无关的额外开销（如请求调度开销、固有网络延迟等）也是一个挑战性的话题。此外，公平的基准测试需要考虑模型输出内容的严格对齐，这在许多测试中常常被忽视。

### 7. 与其他调查的联系

我们对高效生成式LLM服务和推理的调查补充和扩展了该领域现有文献的范围，同时保持了独特的聚焦。在相关研究中，在探索更通用的Transformer模型和领域特定加速器的设计方面与我们最为接近。然而，我们的调查通过专注于生成式LLM服务这一细分领域而有所区别，这是其他研究的中央焦点未覆盖的领域。此外，一些研究深入探讨了GPU上的LLM推理效率的实验研究和新型加速器，提供了与我们服务效率聚焦直接相关的有价值的实证见解。此外，LLMCarbon处理了LLM部署的一个日益重要的方面——其环境影响（例如，碳足迹）。虽然我们调查的主要重点是从性能角度的效率，但此类研究提供的环境视角无疑在我们更广泛的讨论中具有相关性和应受尊重。一些调查和基准测试提供了关于模型压缩和量化的有价值见解。这些研究为我们对相关方向的探索奠定了基础。一些研究为理解LLM的有效性（例如，准确性、困惑度、真实性等）提供了重要的背景，而这些超出了本调查的范围。我们的调查还承认了之前专注于大规模深度神经网络模型分布式训练的调查的贡献，因为它们为考虑LLM服务提供了背景。总之，我们的调查处于一系列多样化研究的中心，借鉴并贡献于对LLM服务效率的更全面理解，包括算法创新和系统优化。通过整合来自这些不同领域的见解，我们旨在提供对该领域最新进展和挑战的细致而全面的概述。

### 8. 7 未来方向

随着我们站在大语言模型（LLM）发展的前沿，不仅理解这些技术的现状，而且预见并塑造其未来轨迹变得越来越重要。特别是在生成式LLM服务领域，存在着广阔的未探索的可能性和新兴挑战。这一领域的快速发展需要一种前瞻性的方法，其中识别创新和改进的潜在途径至关重要。这种远见不仅使我们能够适应即将到来的技术变革，还指导研究界解决最相关和最具影响力的领域。在此背景下，我们概述了几个有前景的未来研究和开发方向，每个方向都有潜力显著提高生成式LLM的服务效率。

• **硬件加速器的开发与增强**

未来提高生成式LLM服务效率的进展可能主要由专用硬件加速器的开发和改进驱动，辅之以硬件和软件优化的协同设计方法。例如，将内存更靠近处理单元或优化芯片架构以更好地与LLM算法的数据流对齐，可以显著减少延迟和能耗。这种方法在最近的GPU进展中得到了体现，如NVIDIA的Hopper架构，它展示了HBM和SRAM容量、内存带宽、计算单元和二分带宽的改进，直接受益于LLM的处理。这一领域的持续创新可能涉及设计本质上与生成式LLM的计算模式相匹配的硬件，例如优化这些模型中普遍存在的注意力机制和张量操作的特定需求，最终影响LLM服务系统的设计和实现。

• **高效且有效的解码算法**

开发更高效的解码算法可以显著提高服务效率。受限于更高效利用LLM中封装的大量知识的需求，未来的工作可以探索传统自回归方法的替代方法，并在保持解码质量的同时解锁实时应用的生成速度。一个有前景的方向是广义推测推理，因为它能够保持相同的生成质量。具体来说，小型推测模型可以推广到任何其他形式的方法，这些方法可以比LLM更高效地生成草稿标记，例如知识检索器和用户定义函数。例如，最近出现了一些后续工作，用早期退出 或非自回归解码 替代草稿模型。总之，像推测解码这样的高效解码算法的开发，结合底层系统优化，代表了提高生成式LLM服务效率的重要机会。

• **长上下文/序列场景优化**

随着LLM的应用继续扩展到更复杂的场景，处理更长上下文或序列的需求稳步增长。为长序列工作负载服务LLM需要从算法和系统两方面解决挑战。就LLM而言，当序列比训练期间观察到的更长时，它们通常会遭受长度泛化失败，即使启用了相对位置编码或在更长的语料库上进行微调。即使是一些声称支持超长上下文的模型，研究发现它们也会遇到“中间丢失”的情况。当前的方法试图通过减少计算序列长度同时保留相关信息来缓解这些限制，例如检索增强、序列压缩和缓存。对于LLM服务系统，更长的序列带来了关键挑战，包括更多的内存消耗和KV缓存的访问，以及自注意力的二次增加的计算复杂性。

• **探索替代架构**

尽管Transformer模型和自注意力机制目前主导了LLM的格局，但探索替代架构是未来研究的一个有前景的方向。深度学习领域历史上见证了主导架构的不断交替，每一次新的范式转变都带来了显著的进步。鉴于这一趋势，重要的是考虑其他架构方法，这些方法可能提供独特的优势，特别是在提高计算效率方面。例如，最近的一些研究探索了无注意力方法，使用纯MLP（多层感知器）架构来替代注意力机制。DNN模型架构的演变不仅是一个自然的进程，而且也是揭示更高效和有效的LLM结构方式的必要探索。

• **复杂环境中的部署探索**

随着LLM应用的扩展，一个关键的未来方向涉及探索和优化其在各种复杂环境中的部署。这一探索超越了传统的基于云的部署，包括边缘计算、混合计算（结合云和边缘计算）、去中心化计算以及利用更经济的资源（如spot实例）等场景。每种环境都为LLM服务带来了独特的挑战和机遇。例如，边缘计算通过更接近数据源处理数据，允许更快的响应时间和减少带宽使用，但它也带来了计算资源和存储容量有限的挑战。**混合计算**提供了一种平衡的方法，但需要先进的管理来高效分配计算任务。去中心化计算为众包计算资源提供了一个有前景的途径，但它也带来了关于数据隐私和安全的额外考虑。在抢占式资源上服务LLM可以显著降低货币成本，但需要容错机制来处理其固有的不可预测性和可变性，确保一致的性能和系统可靠性。成功应对这些复杂环境的挑战将是实现更健壮、可扩展和高效的LLM应用的关键。

• **自动适应特定需求**

多样化的应用特定需求创造了广泛的创新LLM服务优化机会，例如**参数高效微调**、**从外部向量存储中检索**、**在线学习和知识更新**、**多模态工作负载**以及将不同LLM的能力链接在一起。这些独特的挑战还要求通过将优化空间扩展到整个LLM生命周期，包括数据采集和处理、AutoML和模型管理、资源分配和性能监控，自动且平滑地将LLM服务技术集成到现有的IT基础设施中。

### 9 结论

高效的LLM服务是民主化访问先进AI技术的基本步骤。本调查旨在为研究人员、从业者和开发者提供对现有方法的全面理解，使他们在现实环境中部署LLM时能够做出明智的决策。通过整合算法和系统的最新研究成果，本调查论文希望加速进展并促进创新，以追求高效的LLM服务解决方案。

---

## 参考引用

### 书籍出处

- [2023-Towards Efficient Generative Large Language Model Serving](../../../../asset/LLM/paper/2023-Towards%20Efficient%20Generative%20Large%20Language%20Model%20Serving.pdf)

### 网页链接

- [LLM模型部署综述 - 知乎](https://zhuanlan.zhihu.com/p/688282337)