# 模型量化简介

## 介绍 Introduction

卷积神经网络应用的领域很广，但其计算量一般都非常大，如果想应用于嵌入式设备中(如智能手机、无人机、机器人、自动驾驶)，这些设备通常对神经网络的执行过程有着严格的时间限制或者在长期执行时对功耗有则严苛的要求，那么降低延迟和功耗则是一个需要解决的问题。一种有效的降低计算时间和计算量的方法便是**模型量化**，即权重和激活向量由**高比特**向**低比特**保存(如32bit->4bit)，即将权重由训练时的**高精度**转换为**低精度**保存。模型由高精度转换为低精度的过程中，如int8量化(32bit->8bit)，模型存储消耗降低为原来的 $\frac 14$，矩阵乘法的消耗则降低为原来的 $\frac 1{16}$。
当然，模型量化通常还可以和一些常见的网络优化方法一起使用，如 **神经网络结构搜索(NAS)**、**模型压缩(compression)**、**模型剪枝(pruning)** 等。然而，模型量化在降低计算量的同时，由于其较低的量化位宽，可能给网络带来噪声，从而导致模型的**精度下降**。

量化分为两大类方法，**训练后量化(Post-Training Quantization, PTQ)**和**量化感知训练(Quantization-Aware-Training, QAT)**。
- 训练后量化：不需要重新训练，同时只需要很少的数据或者不需要数据(这部分数据集称为校准集)，少部分需要手动调整的超参数以及不需要端到端训练。
- 量化感知训练：需要网络在训练过程中进行模拟量化，且需要进行重新训练以及微调超参数，但是在低精度时却可以比训练后量化获得更接近全精度的效果。

## 量化的基础知识 Quantization fundamentals

### 硬件背景 Hardware background 

首先量化网络是如何在神经网络加速器上实现高效推理的。以矩阵-向量乘法 $y=Wx+b$ 举例，这个结构一般是卷积神经网络或者更大的矩阵-向量计算中的一个基本模块。这是因为AI计算加速模块(NPU、GPU)希望通过尽可能多的并行计算来提高网络的推理效率。这种加速器分为 **处理单元(processing elements)** 和 **累加器(accumulators)**。下图即为16个处理单元和4个累加器组成的基本模块。计算过程如下：

![矩阵计算逻辑](/images/Model_Accelaration/matrix-multiply%20logic%20for%20accelerator%20hardware.png)

如上图所示，计算过程为：

- 累加器首先加载偏置值 $b_n$ ;
- 将权重值 $W_{n,m}$ 和输入值 $x_m$ 加载到数组中，并在单个循环中计算它们的乘积 $C_{n,m}$ =  $W_{n,m} \cdot x_m$ 
- 将**乘积和**与累加器中的偏置值 $b_n$ 进行累加，公式如下：
$$A_n=b_n+\sum_m C_{n,m} \tag{1}$$

上面的操作也可以被称为**乘累加**(Multiply-Accumulate, MAC)。对于更大的矩阵-向量乘法，这个过程将会被多次执行。一旦所有的循环执行完成，累加器中的结果将会被写回内存中，以便被神经网络的下一层所使用。乘累加操作和数据传输(加载和写回等)占据了神经网络推理过程中的大部分消耗，因此使用低精度量化(如INT8量化)可以显著地降低计算量和功耗。


为了将训练网络从浮点数运算转换为整数运算，需要一个转换公式：

$$\widehat{x}=s_x \cdot x_{int} \approx x \tag{2}$$

其中 $S_x$ 是浮点比例因子，$x_{int}$ 是整数向量(如 INT8)。通过量化权重和激活函数，可以写出量化版本的累加方程：

$$\begin{align*}
\widehat{A}_n&=\widehat{b}_n + \sum_m \widehat{W}_{n,m}\cdot\widehat{x}_m\\
&=\widehat{b}_n+\sum_{m} (s_W W_{n,m}^{int}) \cdot (s_x x_m^{int})\\
&=\widehat{b}_n+s_Ws_x\sum_{m} W_{n,m}^{int} \cdot x_m^{int}\\
\end{align*}$$

$$\begin{align*}
\widehat{A}&=\widehat{b} + \\
&=\widehat{b}+\sum_{m} (s_W W_{n,m}^{int}) \cdot (s_x x_m^{int})\\
&=\widehat{b}+s_Ws_x\sum_{m} W_{n,m}^{int} \cdot x_m^{int}\\
\end{align*}$$

$$\begin{align*}
\widehat{A}_n&=\widehat{b}_n + \sum_{m}
\end{align*}$$

权重 $S_w$ 和输入向量 $S_x$ 各有一个单独的比例因子，由于每个比例因子都应用于计算过程，因此可以单独设置为一个**比例因子** $A_m$ ，且偏置在大多数时候也不会影响计算结果，因此可将公式进一步简化。

![模型量化模块推理](/images/Model_Accelaration/matrix-multiply%20logic%20for%20quantized%20inference.png)

以INT8量化举例，权重和输入经过公式(2)的计算后，结果为INT32，$A_m$ 也为32位。为了减少数据传输和下一层操作的复杂性，公式(3)输出被重新量化为INT8，这个过程便是**重量化(requantization)**。

### 非对称量化 asymmetric quantization

**非对称量化**也被称为**均匀仿射量化(Uniform affine Quantization)**，由三个量化参数定义：**比例因子** $s$ 、**零点** $z$ 和**位宽** $b$ 。比例因子 $s$ 和零点 $z$ 用于将浮点值映射到整数范围内，其大小取决于位宽 $b$ 。比例因子通常以浮点数表示，同时指代量化器的步长。零点是一个整数，确保真实零点(真实的0)的量化没有错误。

> 在实际操作的时候一般是没有位宽这个选项的，因为大多硬件已经定好了支持8bit还是4bit，不能支持任意位的选择

这三个参数确定后，向量 $x$ 映射到无符号整数网格公式为：

$$x_{int}=clamp(\lfloor{\frac xs}\rceil+z;0,2^b-1)$$

$「.」$被定义为**四舍五入取整**(round-to-neares)，定义为：

$$clamp(x;a,c)=\begin{cases}a,\quad x< a\\x,\quad a\leq x \leq c\\c,\quad x > c\end{cases}$$

为了得到接近真实输入的实数值，定义了一个**反量化**(de-quantization)操作

$$x \approx \widehat{x}=s(x_{int}-z)$$

结合上述两个步骤，定义量化函数 $q(·)$：
$$\widehat{x}=q(x;s,z,b)=s[clamp(\lfloor{\frac xs}\rceil+z;0,2^b-1)-z]$$

反量化还可以定义**量化范围极限** $(q_{min}, q_{max} )$，其中 $q_{min}=-sz$，$q_{max}=s(2^b -1 -z)$。任何超过这个范围的输入x，都将会被截断到这个范围内。这个操作会导致 **截断误差(clipping error)**。如果想减少截断误差，可以通过增大比例因子 $s$ 从而扩大量化范围的方法来实现。然而增大比例因子 $s$ 会导致**舍入误差**(rounding error)增加(舍入误差的范围是 $[-\frac 12s, \frac 12s]$)

### 对称量化 Symmetric uniform quantization

对称量化是非对称量化的简化版本。对称量化将零点 $z$ 限制为真实的0，这样就减少了非对称量化累加操作时对零点 $z$ 的额外计算开销。但由于缺少了偏移量，这限制了整数和浮点数的映射范围。因此选择有符号整数还是无符号整数就很关键：

$$\begin{align*}
\widehat{x}=&s \cdot x_{int}\\
x_{int}=&clamp(\lfloor{\frac xs}\rceil;0,2^b-1),\text{ for unsigned}\\
x_{int}=&clamp(\lfloor{\frac xs}\rceil;-2^{b-1},2^{b-1}-1),\text{ for signed}\\
\end{align*}$$

无符号对称量化非常适用于单尾分布如ReLU激活，有符号对称量化可以被用于**零对称**分布的数据。下图是对称量化和非对称量化的示意图：

![对称量化和非对称量化](/images/Model_Accelaration/Application%20for%20Symmetric%20uniform%20quantization.png)

### 二次幂量化 Power-of-two quantizer

二次幂量化是对称量化的一个特例，其中比例因子被限制为二次幂($s=2^{-k}$)。这种选择可以提升硬件的计算效率，然而比例因子 $s$ 的限制性表达可能会使截断误差和舍入误差之间的权衡变得复杂。

### 量化粒度 Quantization granularity

前文的定义乘累加称为**张量量化**(也称为**按层量化**)，还可以为张量的各个部分(例如权重的输入通道)定义一个单独的量化器(也称为**按通道量化**)，从而提高**量化粒度**。

在模型量化中，**按层量化**是最常见的量化粒度选择，因为它的硬件实现非常简单，如公式(3)使用相同的权重因子 $S_w$ 和输入向量因子 $S_x$。然而可以使用更细腻的量化粒度来进一步提高性能。例如对于权重量化，可以为每个输出通道指定一个不同的量化器。这就是所谓的**按通道量化(per channel)**。甚至还可以使用比按通道量化更细粒度的**按组量化(per-group)**。增加组的粒度通常可以提升量化的准确性，但是要额外付出一些计算开销。额外开销的多少和具体硬件对累加器的实现有关。

## 量化模拟 Quantization simulation

为了测试神经网络在量化设备上的运行情况，会经常在用于训练神经网络的平台上模拟量化行为(如瑞芯微RK3588s的量化框架在Linux服务器的部署)，称之为**量化模拟**(quantization simulation)。目的是使用浮点硬件来近似定点操作。和在实际的硬件或者量化核上进行实验相比，这种模拟操作更容易实现。它允许用户更有效的测试各种量化选项，同时可以使用GPU加速**量化感知训练(quantization-aware training)**。

之前介绍专用定点硬件中执行矩阵-向量乘法的原理，可将这个过程推广到卷积计算中，同时加入激活函数使得更接近真实情况(图4a)。在边缘侧进行推理时，所有的输入(偏置、权重、激活的输入)都是定点格式(如INT8量化)。然而常见的深度学习训练框架和通用的硬件模拟设备在进行这些操作时，都是采用的浮点。这就是为什么要加入量化器来引入量化效果的原因。

图4b展示了如何在深度学习框架中对同一卷积层进行量化模拟的方法。和图4a只有一个地方不同的是，在权重和卷积之间添加量化器来模拟权重的量化，在激活之后添加量化器来模拟激活量化。偏置通常不需要量化，因为它们以更高精度进行存储。

![量化推理](/images/Model_Accelaration/quantized%20forward%20pass%20for%20convolutional%20layer.png)














## 参考引用

- [再读《神经网络量化白皮书》- 0x01 摘要和绪论 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/462976274)
- [再读《神经网络量化白皮书》- 0x02 量化的一些基础知识 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/462990610)
- [再读《神经网络量化白皮书》- 0x03 训练后量化(PTQ) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/463198140)
- [再读《神经网络量化白皮书》- 0x04 训练时量化(QAT) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/467841404)
- [再读《神经网络量化白皮书》- 0x05 总结和结论 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/468124540)
- [再读《神经网络量化白皮书》- 0x06 自己的碎碎念 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/468133773)
- [【已完结】量化神经网络白皮书 2021_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1vf4y1K7km/?spm_id_from=333.337.search-card.all.click&vd_source=08884e3fd559404f507c1ef81f909bfa)
