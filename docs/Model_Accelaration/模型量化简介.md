# 模型量化简介

## 介绍 Introduction

卷积神经网络应用的领域很广，但其计算量一般都非常大，如果想应用于嵌入式设备中(如智能手机、无人机、机器人、自动驾驶)，这些设备通常对神经网络的执行过程有着严格的时间限制或者在长期执行时对功耗有则严苛的要求，那么降低延迟和功耗则是一个需要解决的问题。一种有效的降低计算时间和计算量的方法便是**模型量化**，即权重和激活向量由**高比特**向**低比特**保存(如32bit->4bit)，即将权重由训练时的**高精度**转换为**低精度**保存。模型由高精度转换为低精度的过程中，如int8量化(32bit->8bit)，模型存储消耗降低为原来的 $\frac 14$，矩阵乘法的消耗则降低为原来的 $\frac 1{16}$。
当然，模型量化通常还可以和一些常见的网络优化方法一起使用，如 **神经网络结构搜索(NAS)**、**模型压缩(compression)**、**模型剪枝(pruning)** 等。然而，模型量化在降低计算量的同时，由于其较低的量化位宽，可能给网络带来噪声，从而导致模型的**精度下降**。

量化分为两大类方法，**训练后量化(Post-Training Quantization, PTQ)**和**量化感知训练(Quantization-Aware-Training, QAT)**。
- 训练后量化：不需要重新训练，同时只需要很少的数据或者不需要数据(这部分数据集称为校准集)，少部分需要手动调整的超参数以及不需要端到端训练。
- 量化感知训练：需要网络在训练过程中进行模拟量化，且需要进行重新训练以及微调超参数，但是在低精度时却可以比训练后量化获得更接近全精度的效果。

## 量化的基础知识 Quantization fundamentals

### 硬件背景 Hardware background 

首先量化网络是如何在神经网络加速器上实现高效推理的。以矩阵-向量乘法 $y=Wx+b$ 举例，这个结构一般是卷积神经网络或者更大的矩阵-向量计算中的一个基本模块。这是因为AI计算加速模块(NPU、GPU)希望通过尽可能多的并行计算来提高网络的推理效率。这种加速器分为 **处理单元(processing elements)** 和 **累加器(accumulators)**。下图即为16个处理单元和4个累加器组成的基本模块。计算过程如下：

![矩阵计算逻辑](/images/Model_Accelaration/matrix-multiply%20logic%20for%20accelerator%20hardware.png)

如上图所示，计算过程为：

- 累加器首先加载偏置值 $b_n$ ;
- 将权重值 $W_{n,m}$ 和输入值 $x_m$ 加载到数组中，并在单个循环中计算它们的乘积 $C_{n,m}$ =  $W_{n,m} \cdot x_m$ 
- 将**乘积和**与累加器中的偏置值 $b_n$ 进行累加，公式如下：
- 
$$A_n=b_n+\sum_m C_{n,m} \tag{1}$$

上面的操作也可以被称为**乘累加**(Multiply-Accumulate, MAC)。对于更大的矩阵-向量乘法，这个过程将会被多次执行。一旦所有的循环执行完成，累加器中的结果将会被写回内存中，以便被神经网络的下一层所使用。乘累加操作和数据传输(加载和写回等)占据了神经网络推理过程中的大部分消耗，因此使用低精度量化(如INT8量化)可以显著地降低计算量和功耗。

为了将训练网络从浮点数运算转换为整数运算，需要一个转换公式：

$$\widehat{x}=s_x \cdot x_{int} \approx x \tag{2}$$

其中 $S_x$ 是浮点比例因子，$x_{int}$ 是整数向量(如 INT8)。通过量化权重和激活函数，可以写出量化版本的累加方程：

$$\begin{align*} 
\widehat{A}_n &= \widehat{b}_n + \sum_m \widehat{W} \cdot \widehat{x}_m \\
&=\widehat{b}_n+\sum_m (s_W W^{int}) \cdot (s_x x_m^{int})\\
&=\widehat{b}_n+ s_W s_x \sum_m W^{int} \cdot x_m^{int} \tag{3}\\
\end{align*}$$



权重 $S_w$ 和输入向量 $S_x$ 各有一个单独的比例因子，由于每个比例因子都应用于计算过程，因此可以单独设置为一个**比例因子** $A_m$ ，且偏置在大多数时候也不会影响计算结果，因此可将公式进一步简化。

![模型量化模块推理](/images/Model_Accelaration/matrix-multiply%20logic%20for%20quantized%20inference.png)

以INT8量化举例，权重和输入经过公式(2)的计算后，结果为INT32，$A_m$ 也为32位。为了减少数据传输和下一层操作的复杂性，公式(3)输出被重新量化为INT8，这个过程便是**重量化(requantization)**。

### 非对称量化 asymmetric quantization

**非对称量化**也被称为**均匀仿射量化(Uniform affine Quantization)**，由三个量化参数定义：**比例因子** $s$ 、**零点** $z$ 和**位宽** $b$ 。比例因子 $s$ 和零点 $z$ 用于将浮点值映射到整数范围内，其大小取决于位宽 $b$ 。比例因子通常以浮点数表示，同时指代量化器的步长。零点是一个整数，确保真实零点(真实的0)的量化没有错误。

> 在实际操作的时候一般是没有位宽这个选项的，因为大多硬件已经定好了支持8bit还是4bit，不能支持任意位的选择

这三个参数确定后，向量 $x$ 映射到无符号整数网格公式为：

$$x_{int}=clamp(\lfloor{\frac xs}\rceil+z;0,2^b-1)$$

$「.」$被定义为**四舍五入取整**(round-to-neares)，定义为：

$$clamp(x;a,c)=
\begin{cases}
a,\quad x< a \\ 
x,\quad a\leq x \leq c \\
c,\quad x > c
\end{cases}$$

为了得到接近真实输入的实数值，定义了一个**反量化**(de-quantization)操作

$$x \approx \widehat{x}=s(x_{int}-z)$$

结合上述两个步骤，定义量化函数 $q(·)$：
$$\widehat{x}=q(x;s,z,b)=s[clamp(\lfloor{\frac xs}\rceil+z;0,2^b-1)-z]$$

反量化还可以定义**量化范围极限** $(q_{min}, q_{max} )$，其中 $q_{min}=-sz$，$q_{max}=s(2^b -1 -z)$。任何超过这个范围的输入x，都将会被截断到这个范围内。这个操作会导致 **截断误差(clipping error)**。如果想减少截断误差，可以通过增大比例因子 $s$ 从而扩大量化范围的方法来实现。然而增大比例因子 $s$ 会导致**舍入误差**(rounding error)增加(舍入误差的范围是 $[-\frac 12s, \frac 12s]$)

### 对称量化 Symmetric uniform quantization

对称量化是非对称量化的简化版本。对称量化将零点 $z$ 限制为真实的0，这样就减少了非对称量化累加操作时对零点 $z$ 的额外计算开销。但由于缺少了偏移量，这限制了整数和浮点数的映射范围。因此选择有符号整数还是无符号整数就很关键：

$$\begin{align*}
\widehat{x}=&s \cdot x_{int}\\
x_{int}=&clamp(\lfloor{\frac xs}\rceil;0,2^b-1),\text{ for unsigned}\\
x_{int}=&clamp(\lfloor{\frac xs}\rceil;-2^{b-1},2^{b-1}-1),\text{ for signed}\\
\end{align*}$$

无符号对称量化非常适用于单尾分布如ReLU激活，有符号对称量化可以被用于**零对称**分布的数据。下图是对称量化和非对称量化的示意图：

![对称量化和非对称量化](/images/Model_Accelaration/Application%20for%20Symmetric%20uniform%20quantization.png)

### 二次幂量化 Power-of-two quantizer

二次幂量化是对称量化的一个特例，其中比例因子被限制为二次幂($s=2^{-k}$)。这种选择可以提升硬件的计算效率，然而比例因子 $s$ 的限制性表达可能会使截断误差和舍入误差之间的权衡变得复杂。

### 量化粒度 Quantization granularity

前文的定义乘累加称为**张量量化**(也称为**按层量化**)，还可以为张量的各个部分(例如权重的输入通道)定义一个单独的量化器(也称为**按通道量化**)，从而提高**量化粒度**。

在模型量化中，**按层量化**是最常见的量化粒度选择，因为它的硬件实现非常简单，如公式(3)使用相同的权重因子 $S_w$ 和输入向量因子 $S_x$。然而可以使用更细腻的量化粒度来进一步提高性能。例如对于权重量化，可以为每个输出通道指定一个不同的量化器。这就是所谓的**按通道量化(per channel)**。甚至还可以使用比按通道量化更细粒度的**按组量化(per-group)**。增加组的粒度通常可以提升量化的准确性，但是要额外付出一些计算开销。额外开销的多少和具体硬件对累加器的实现有关。

## 量化模拟 Quantization simulation

为了测试神经网络在量化设备上的运行情况，会经常在用于训练神经网络的平台上模拟量化行为(如瑞芯微RK3588s的量化框架在Linux服务器的部署)，称之为**量化模拟**(quantization simulation)。目的是使用浮点硬件来近似定点操作。和在实际的硬件或者量化核上进行实验相比，这种模拟操作更容易实现。它允许用户更有效的测试各种量化选项，同时可以使用GPU加速**量化感知训练(quantization-aware training)**。

之前介绍专用定点硬件中执行矩阵-向量乘法的原理，可将这个过程推广到卷积计算中，同时加入激活函数使得更接近真实情况(图4a)。在边缘侧进行推理时，所有的输入(偏置、权重、激活的输入)都是定点格式(如INT8量化)。然而常见的深度学习训练框架和通用的硬件模拟设备在进行这些操作时，都是采用的浮点。这就是为什么要加入量化器来引入量化效果的原因。

图4b展示了如何在深度学习框架中对同一卷积层进行量化模拟的方法。和图4a只有一个地方不同的是，在权重和卷积之间添加量化器来模拟权重的量化，在激活之后添加量化器来模拟激活量化。偏置通常不需要量化，因为它们以更高精度进行存储。

![量化推理](/images/Model_Accelaration/quantized%20forward%20pass%20for%20convolutional%20layer.png)

### 批归一化折叠 Batch normalization folding

批归一化是卷积神经网络的一个基本模块，其对线性输出层进行归一化然后缩放和加入偏置。在边缘侧推理时，这个操作可以被融合到前一个或者后一个线性层中去，这就是**批归一化折叠(batch normalization folding)**。这相当于从网络中完全删除了批归一操作，被吸收到相邻的线性层之中。除了减少额外的缩放和偏移计算，这个操作还可以省去额外的数据搬移和输出层的量化。[批归一化原理](/docs/Model_Networks/Normalization.md#批归一化-batch-normalization)

- 假设全连接层为：

$$ y = W \cdot X + B \tag{4}$$

- BN层计算：
  
$$\begin{align*} 
\mu &= \frac{1}{m} \sum_{i=1}^m x_i \\
\sigma^2 &= \frac{1}{m} \sum_{i=1}^m (x_i - \mu)^2 \\
\hat{x_i} &= \frac{x_i - \mu}{\sqrt {\sigma^2 + \epsilon}} \\
y_i &= \gamma \cdot \hat{x_i} + \beta
\end{align*}$$

- BN层折叠：

$$\begin{align*} 
y_k &= BatchNorm(W_k \cdot X + b) \\
&= \gamma_k \cdot \hat{x} + \beta \\
&= \gamma_k (\frac{(W_k x + B) - \mu_k}{\sqrt {\sigma^2 + \epsilon}}) + \beta_k \\
&= \frac{\gamma_k W_k x}{\sqrt {\sigma_k^2 + \epsilon}} + (\beta_k + \frac{\gamma_k (B - \mu_k)}{\sqrt {\sigma_k^2 + \epsilon}}) \\
&= \widetilde{W_k} \cdot X + \widetilde{b_k} \tag{5}
\end{align*} $$

- 合并BN层后的卷积层的权重和偏置：

$$\begin{align*} 
A &= \frac{\gamma_k}{\sqrt {\sigma_k^2 + \epsilon}} \\
\widetilde{W_k} &=  \frac{\gamma_k W_k}{\sqrt {\sigma_k^2 + \epsilon}} = W_k \cdot A \tag{6} \\
\widetilde{b_k} &= \beta_k + \frac{\gamma_k (B - \mu_k)}{\sqrt {\sigma_k^2 + \epsilon}} = (B - \mu_k) \cdot A + \beta_k \tag{7}
\end{align*}$$

从而就将BN层融入到了全连接层的参数中，当不计算偏置项时，可令 $B=0$ 从而进一步简化公式

### 激活函数融合 Activation function fusing

前面介绍量化加速器时，反量化是在矩阵乘法或者卷积计算之后进行的。然而在实际情况中，在线性操作后通常会有一个非线性操作，即在卷积之后一般会有激活函数。如果将线性层的结果写到内存，然后又加载到非线性层中进行计算，这个操作是很消耗时间的。因此，许多硬件实现都会在反量化之前应用非线性操作，只需要模拟非线性操作之后的反量化操作即可。例如RELU的非线性操作就很容易被反量化模块所模拟，只需要将激活的最小值量化值设置为0即可

> 假设RELU的正半轴是乘以1，也就是没有操作。负半轴是截断为0，也就是clip到0。所以把量化的最小值设为0，最大值按照正常操作计算得到。这样自然就实现了去掉负半轴的操作，也就是RELU操作。

### 其它层量化 Other layers and quantization

神经网络中还使用了许多其他类型的层。如何对这些层进行建模，在很大程度上取决于具体的硬件实现。有时模拟量化和目标性能之间的不匹配就是因为这些层没有被正确量化。

- **最大池化层(Max pooling)**：不需要量化，因为输入和输出范围一致
- **平均池化(Average pooling)**：整数的平均数不一定是整数，因此需要在平均数之后增加一个量化步骤。但是对输入和输出使用相同的量化器，因为求平均数不会显著改变量化后值的范围。
- **逐点相加(Element-wise addition)**：尽管计算行为很简单，但是对这个操作确很难准确的进行模拟。在计算的时候两个输入的量化范围必须要完全匹配。如果输入的量化范围不匹配，就需要格外的注意才能确保计算能正确的执行。
- **拼接(Concatenation)**：被连接的两个分支(两个是泛指)通常不共享量化参数，这意味着它们的量化范围不一定会重叠，因此反量化步骤可能是需要的。与逐点相加一样，可以对网络进行微调使多个连接分支可以共享量化参数。

## 实际应用 Practical considerations

在对神经网络进行量化时，会面临很多量化方式的选择，包括量化方案、细粒度和位宽。

### 对称和非对称量化 Symmetric vs. asymmetric quantization

对于所有的权重和输入，非对称量化可以有更好的表达能力，因为使用了相对零点这个参数，但会使计算量更大。参考公式(3)，非对称量化的权重和输入相乘时:

$$\begin{align*} 
\widehat{W} \cdot \widehat{x} &= s_W(W_i-z_W)s_x(x_i-z_x) \\
&= s_W s_x W_i x_i - s_W z_W s_x x_i -s_W s_x z_x W_i + s_W z_W s_x z_x \tag{8}
\end{align*}$$

权重和输入都采用对称量化时，即不使用相对零点，则只有第一项。使用非对称量化时，第三和第四项只取决于比例 $s$、零点 $z$ 和权重值，因此这两个部分可以提前计算出结果，添加到对应层的偏置中，在推理时不需要额外计算。第二项取决于输入数据 $x$，每次计算都会导致更大的延迟和功耗开销。因此，对输入使用非对称量化，对权重使用对称量化是一种比较常见的方法，这可以避免额外的数据依赖和计算量。

### 按层量化还是按通道量化 Per-tensor and per-channel quantization

前文提到的量化粒度中，按层量化因为所有加速器都支持这种量化方法得到广泛应用。然而权重分布在各个通道之间差异比较大的时候，按通道量化可以提高准确性。如公式(3)，对权重按通道量化可以通过对每个权重通道使用不同的缩放系数来实现，而不需要对整体进行缩放操作。对输入按通道量化则很难实现，因为无法将缩放系数从求和中简单的分解出来，因此需要对每个输入加法器的数据通道进行单独的重新调整。虽然权重按通道量化越来越普遍，但并不是所有的加速器硬件都支持这种操作。

## 训练后量化 Post-training quantization 

**训练后量化算法(PTQ)** 不需要重新训练，就能将预训练的FP32网络直接转换为定点网络 (如INT8量化)。这个方法通常不需要数据 (即data-free) 或者只需要很少的**校准数据集**，这部分数据一般比较容易获取 (只要大概合适的图片就行，不需要带标签)。由于这种方法几乎不需要调整超参数，可以高效地对预训练的神经网络模型权重进行量化，从而让神经网络量化的应用变得更加广泛。

### 量化范围的设置 Quantization range setting

量化范围设置是指确定量化范围的截断阈值 $q_{min}$ 和 $q_{max}$ 的方法。权重通常可以在不需要校准数据的情况下进行量化。然而确定激活量化的参数通常需要几批校准数据 (因为训练完成后模型的权重就固定了，但是激活是和输入有关的)。

- **绝对值量化**：$V$ 表示要量化的张量。这种方法对异常值很敏感，因为异常值可能会导致过多的舍入误差 (比如最小值比最大值多出一大截，这部分精度就浪费掉了)

$$q_ $$







## 参考引用

### 论文地址



### 博客

- [【已完结】量化神经网络白皮书 2021_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1vf4y1K7km/?spm_id_from=333.337.search-card.all.click&vd_source=08884e3fd559404f507c1ef81f909bfa)
- [再读《神经网络量化白皮书》- 0x01 摘要和绪论 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/462976274)
- [再读《神经网络量化白皮书》- 0x02 量化的一些基础知识 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/462990610)
- [再读《神经网络量化白皮书》- 0x03 训练后量化(PTQ) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/463198140)
- [再读《神经网络量化白皮书》- 0x04 训练时量化(QAT) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/467841404)
- [再读《神经网络量化白皮书》- 0x05 总结和结论 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/468124540)
- [再读《神经网络量化白皮书》- 0x06 自己的碎碎念 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/468133773)
- [深度学习算法优化系列十一 | 折叠Batch Normalization](https://zhuanlan.zhihu.com/p/107913057)