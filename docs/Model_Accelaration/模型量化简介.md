# 模型量化简介

## 介绍 Introduction

随着深度学习的发展，神经网络被广泛应用于各种领域，这些模型为了保证较高的精度，大部分运算都是采用浮点型进行计算，常见的是32位浮点型和64位浮点型，即 Float32 和 Double64，准确率高但计算量和参数量一般都非常大，如果想应用于嵌入式设备中(如智能手机、无人机、机器人、自动驾驶)，这些设备通常对神经网络的执行过程有着严格的时间限制、内存制约或者在长期执行时对功耗有则严苛的要求，那么降低延迟和功耗及模型大小同时又不损失过多的检测精度则是一个需要解决的问题。一种有效的降低模型大小、计算时间和计算量的方法便是**模型量化**，即权重和激活向量由**高比特**向**低比特**保存(如32bit->4bit)，将权重由训练时的**高精度**转换为**低精度**。模型由高精度转换为低精度的过程中，如INT8量化(32bit->8bit)，模型存储消耗降低为原来的 $\frac 14$，矩阵乘法的消耗则降低为原来的 $\frac 1{16}$。**模型量化本质上只是对数值范围的重新调整，可以粗略理解为是一种线性映射**。

![模型量化优势](/images/Model_Accelaration/模型量化优势.png)

当然，模型量化通常还可以和一些常见的网络优化方法一起使用，如 **神经网络结构搜索(NAS)**、**模型压缩(compression)**、**模型剪枝(pruning)** 等。然而，模型量化在降低计算量的同时，由于其较低的量化位宽，可能给网络带来噪声，从而导致模型的**精度下降**。

量化分为两大类方法，**训练后量化(Post-Training Quantization, PTQ)**和**量化感知训练(Quantization-Aware-Training, QAT)**。
- **训练后量化**：**对预训练后的网络选择合适的量化操作和校准操作，以实现量化损失的最小化**。该过程不需要重新训练，同时只需要很少的数据或者不需要数据(这部分数据集称为校准集)及少部分需要手动调整的超参数。
- **量化感知训练**：需要网络在训练过程中进行模拟量化，且需要进行重新训练以及微调超参数，但是在低精度时却可以比训练后量化获得更接近全精度的效果。

## 量化的基础知识 Quantization fundamentals

### 硬件背景 Hardware background 

首先量化网络是如何在神经网络加速器上实现高效推理的。以矩阵-向量乘法 $y=Wx+b$ 举例，这个结构一般是卷积神经网络或者更大的矩阵-向量计算中的一个基本模块。这是因为AI计算加速模块(NPU、GPU)希望通过尽可能多的并行计算来提高网络的推理效率。这种加速器分为 **处理单元(processing elements)** 和 **累加器(accumulators)**。下图即为16个处理单元和4个累加器组成的基本模块。计算过程如下：

![矩阵计算逻辑](/images/Model_Accelaration/matrix-multiply%20logic%20for%20accelerator%20hardware.png)

如上图所示，计算过程为：

- 累加器首先加载偏置值 $b_n$ ;
- 将权重值 $W_{n,m}$ 和输入值 $x_m$ 加载到数组中，并在单个循环中计算它们的乘积 $C_{n,m}$ =  $W_{n,m} \cdot x_m$ 
- 将**乘积和**与累加器中的偏置值 $b_n$ 进行累加，公式如下：
  
$$A_n=b_n+\sum_m C_{n,m} \tag{1}$$

上面的操作也可以被称为**乘累加**(Multiply-Accumulate, MAC)。对于更大的矩阵-向量乘法，这个过程将会被多次执行。一旦所有的循环执行完成，累加器中的结果将会被写回内存中，以便被神经网络的下一层所使用。乘累加操作和数据传输(加载和写回等)占据了神经网络推理过程中的大部分消耗，因此使用低精度量化(如INT8量化)可以显著地降低计算量和功耗。

为了将训练网络从浮点数运算转换为整数运算，需要一个转换公式：

$$\widehat{x}=s_x \cdot x_{int} \approx x \tag{2}$$

其中 $S_x$ 是浮点比例因子，$x_{int}$ 是整数向量(如 INT8)。通过量化权重和激活函数，可以写出量化版本的累加方程：

$$\begin{align*} 
\widehat{A}_n &= \widehat{b}_n + \sum_m \widehat{W} \cdot \widehat{x}_m \\
&=\widehat{b}_n+\sum_m (s_W W^{int}) \cdot (s_x x_m^{int})\\
&=\widehat{b}_n+ s_W s_x \sum_m W^{int} \cdot x_m^{int} \tag{3}\\
\end{align*}$$



权重 $S_w$ 和输入向量 $S_x$ 各有一个单独的比例因子，由于每个比例因子都应用于计算过程，因此可以单独设置为一个**比例因子** $A_m$ ，且偏置在大多数时候也不会影响计算结果，因此可将公式进一步简化。

![模型量化模块推理](/images/Model_Accelaration/matrix-multiply%20logic%20for%20quantized%20inference.png)

以INT8量化举例，权重和输入经过公式(2)的计算后，结果为INT32，$A_m$ 也为32位。为了减少数据传输和下一层操作的复杂性，公式(3)输出被重新量化为INT8，这个过程便是**重量化(requantization)**。

### 线性量化 Uniform Quantization

#### 非对称量化 Asymmetric Quantization

**非对称量化**也被称为**均匀仿射量化(Uniform affine Quantization)**，由三个量化参数定义：**比例因子** $s$ 、**零点** $z$ 和**位宽** $b$ 。比例因子 $s$ 和零点 $z$ 用于将浮点值映射到整数范围内，其大小取决于位宽 $b$，**即使用一个映射公式将输入数据映射到 $[0, 2^b-1]$ 范围内**。比例因子通常以浮点数表示，同时指代量化器的步长。零点是一个整数，确保真实零点(真实的0)的量化没有错误。

> 在实际操作的时候一般是没有位宽这个选项的，因为大多硬件已经定好了支持8bit还是4bit，不能支持任意位的选择

这三个参数确定后，向量 $x$ 映射到无符号整数网格公式为：

$$x_{int}=clamp(\lfloor{\frac xs}\rceil+z;0,2^b-1) \tag{4}$$

$「.」$被定义为**四舍五入取整**(round-to-neares)，定义为：

$$clamp(x;a,c)=
\begin{cases}
a,\quad x< a \\ 
x,\quad a\leq x \leq c \\
c,\quad x > c
\end{cases}$$

为了得到接近真实输入的实数值，定义了一个**反量化**(de-quantization)操作

$$x \approx \widehat{x}=s(x_{int}-z) \tag{5}$$

结合上述两个步骤，定义量化函数 $q(·)$: 

$$\widehat{x}=q(x;s,z,b)=s[clamp(\lfloor{\frac xs}\rceil+z;0,2^b-1)-z] \tag{6}$$

反量化还可以定义**量化范围极限** $(q_{min}, q_{max} )$，其中 $q_{min}=-sz$，$q_{max}=s(2^b -1 -z)$。任何超过这个范围的输入x，都将会被截断到这个范围内。这个操作会导致 **截断误差(clipping error)**。如果想减少截断误差，可以通过增大比例因子 $s$ 从而扩大量化范围的方法来实现。然而增大比例因子 $s$ 会导致**舍入误差**(rounding error)增加(舍入误差的范围是 $[-\frac 12s, \frac 12s]$)

从上面公式可以明显看出，**反量化一般没有信息损失，而量化一般都会有精度损失**。浮点数能保存的数值范围本身就比 整数多，因此必定有大量数值无法用整数表示，只能四舍五入成整型数值

#### 对称量化 Symmetric Uniform Quantization

对称量化是非对称量化的简化版本。对称量化将零点 $z$ 限制为真实的0，这样就减少了非对称量化累加操作时对零点 $z$ 的额外计算开销。但由于缺少了偏移量，这限制了整数和浮点数的映射范围。因此选择有符号整数还是无符号整数就很关键：

$$\begin{align*}
\widehat{x}=&s \cdot x_{int}\\
x_{int}=&clamp(\lfloor{\frac xs}\rceil;0,2^b-1),\text{ for unsigned}\\
x_{int}=&clamp(\lfloor{\frac xs}\rceil;-2^{b-1},2^{b-1}-1),\text{ for signed}\\
\end{align*}$$

无符号对称量化非常适用于单尾分布如ReLU激活，有符号对称量化可以被用于**零对称**分布的数据。下图是对称量化和非对称量化的示意图：

![对称量化和非对称量化](/images/Model_Accelaration/Application%20for%20Symmetric%20uniform%20quantization.png)

### 非线性量化

#### 二次幂量化 Power-of-two Quantizer

二次幂量化是对称量化的一个特例，其中比例因子被限制为二次幂($s=2^{-k}$)。这种选择可以提升硬件的计算效率，然而比例因子 $s$ 的限制性表达可能会使截断误差和舍入误差之间的权衡变得复杂。

#### 加法项二次幂量化 Additive Power-of-two

https://zhuanlan.zhihu.com/p/349678095

### 量化粒度 Quantization Granularity

量化粒度是指选取多少个待量化参数共享一个量化系数，通常来说粒度越大，精度损失越大。前文定义的乘累加称为**张量量化**(也称为**按层量化**)，还可以为张量的各个部分(例如权重的输入通道)定义一个单独的量化器(也称为**按通道量化**)，从而提高**量化粒度**。量化的量化粒度越小，模型的精度损失越小，但是计算量也相应更大。

- **按层量化**：最常见的量化粒度选择，因为它的剪裁范围是通过每一层卷积滤波器中的所有权重来确定的且硬件实现非常简单，如公式(3)使用相同的权重因子 $S_w$ 和输入向量因子 $S_x$ 然后对所有卷积滤波器使用相同的剪裁范围，如图3的第三列所示。虽然这种方法实现起来非常简单，但通常会陷入局部最优。
- **按通道量化(per channel)**：对于权重量化，可以为每个输出通道指定一个不同的量化器，即对每个卷积滤波器使用固定值，如图3的第四列所示。这钟方法有更好的量化分辨率且准确性更高，是**目前用于量化卷积核的标准方法**。
- **按组量化(per-group)**。将层内多个不同通道分组以计算剪裁范围，但需要针对性的考虑不同组的缩放因子

![Illustration of different quantization granularities](/images/Model_Accelaration/Illustration%20of%20different%20quantization%20granularities.png)

## 量化仿真 Quantization Simulation

为了测试神经网络在量化设备上的运行情况，会经常在用于训练神经网络的平台上模拟量化行为(如瑞芯微RK3588s的量化框架在Linux服务器的部署)，称之为**量化仿真**(quantization simulation)。

量化仿真可以在你的训练服务器上而不是目标设备（手机，嵌入式设备等）上进行量化模型效果测试，不需要专用的硬件内核就可以完成模型的测试，并且能够灵活方便的选择不同的量化位宽和量化算法。它允许用户更有效的测试各种量化选项，同时可以使用GPU加速**量化感知训练(quantization-aware training)**提升测试效率。

之前介绍专用定点硬件中执行矩阵-向量乘法的原理，可将这个过程推广到卷积计算中，同时加入激活函数使得更接近真实情况(图4a)。在边缘侧进行推理时，所有的输入(偏置、权重、激活的输入)都是定点格式(如INT8量化)。然而常见的深度学习训练框架和通用的硬件模拟设备在进行这些操作时，都是采用的浮点。这就是为什么要加入量化器来引入量化效果的原因。

图4b展示了如何在深度学习框架中对同一卷积层进行量化模拟的方法。和图4a只有一个地方不同的是，在权重和卷积之间添加量化器来模拟权重的量化，在激活之后添加量化器来模拟激活量化。偏置通常不需要量化，因为它们以更高精度进行存储。

![量化推理](/images/Model_Accelaration/quantized%20forward%20pass%20for%20convolutional%20layer.png)

### 批归一化折叠 Batch Normalization Folding

批归一化折叠也称为 **BN 融合**，是卷积神经网络的一个基本模块，其对线性输出层进行归一化然后缩放和加入偏置。在边缘侧推理时，这个操作可以被融合到前一个或者后一个线性层中去，这就是**批归一化折叠(batch normalization folding)**。这相当于从网络中完全删除了批归一操作，被吸收到相邻的线性层之中。除了减少额外的缩放和偏移计算，这个操作还可以省去额外的数据搬移和输出层的量化。[批归一化原理](/docs/Model_Networks/Normalization.md#批归一化-batch-normalization)

- 假设全连接层为：

$$ y = W \cdot X + B \tag{7}$$

- BN层计算：
  
$$\begin{align*} 
\mu &= \frac{1}{m} \sum_{i=1}^m x_i \\
\sigma^2 &= \frac{1}{m} \sum_{i=1}^m (x_i - \mu)^2 \\
\hat{x_i} &= \frac{x_i - \mu}{\sqrt {\sigma^2 + \epsilon}} \\
y_i &= \gamma \cdot \hat{x_i} + \beta
\end{align*}$$

- BN层折叠：

$$\begin{align*} 
y_k &= BatchNorm(W_k \cdot X + b) \\
&= \gamma_k \cdot \hat{x} + \beta \\
&= \gamma_k (\frac{(W_k x + B) - \mu_k}{\sqrt {\sigma^2 + \epsilon}}) + \beta_k \\
&= \frac{\gamma_k W_k x}{\sqrt {\sigma_k^2 + \epsilon}} + (\beta_k + \frac{\gamma_k (B - \mu_k)}{\sqrt {\sigma_k^2 + \epsilon}}) \\
&= \widetilde{W_k} \cdot X + \widetilde{b_k} \tag{8}
\end{align*} $$

- 合并BN层后的卷积层的权重和偏置：

$$\begin{align*} 
A &= \frac{\gamma_k}{\sqrt {\sigma_k^2 + \epsilon}} \\
\widetilde{W_k} &=  \frac{\gamma_k W_k}{\sqrt {\sigma_k^2 + \epsilon}} = W_k \cdot A \tag{9} \\
\widetilde{b_k} &= \beta_k + \frac{\gamma_k (B - \mu_k)}{\sqrt {\sigma_k^2 + \epsilon}} = (B - \mu_k) \cdot A + \beta_k \tag{10}
\end{align*}$$

从而就将BN层融入到了全连接层的参数中，当不计算偏置项时，可令 $B=0$ 从而进一步简化公式

### 激活函数融合 Activation Function Fusing

前面介绍量化加速器时，反量化是在矩阵乘法或者卷积计算之后进行的。然而在实际情况中，在线性操作后通常会有一个非线性操作，即在卷积之后一般会有激活函数。如果将线性层的结果写到内存，然后又加载到非线性层中进行计算，这个操作是很消耗时间的。因此，许多硬件实现都会在反量化之前应用非线性操作，只需要模拟非线性操作之后的反量化操作即可。例如RELU的非线性操作就很容易被反量化模块所模拟，只需要将激活的最小值量化值设置为0即可

> 假设RELU的正半轴是乘以1，也就是没有操作。负半轴是截断为0，也就是c截断到0。所以把量化的最小值设为0，最大值按照正常操作计算得到。这样自然就实现了去掉负半轴的操作，也就是RELU操作。

### 其它层量化 Other Layers and Quantization

神经网络中还使用了许多其他类型的层。如何对这些层进行建模，在很大程度上取决于具体的硬件实现。有时模拟量化和目标性能之间的不匹配就是因为这些层没有被正确量化。

- **最大池化层(Max pooling)**：不需要量化，因为输入和输出范围一致
- **平均池化(Average pooling)**：整数的平均数不一定是整数，因此需要在平均数之后增加一个量化步骤。但是对输入和输出使用相同的量化器，因为求平均数不会显著改变量化后值的范围。
- **逐点相加(Element-wise addition)**：尽管计算行为很简单，但是对这个操作确很难准确的进行模拟。在计算的时候两个输入的量化范围必须要完全匹配。如果输入的量化范围不匹配，就需要格外的注意才能确保计算能正确的执行。
- **拼接(Concatenation)**：被连接的两个分支(两个是泛指)通常不共享量化参数，这意味着它们的量化范围不一定会重叠，因此反量化步骤可能是需要的。与逐点相加一样，可以对网络进行微调使多个连接分支可以共享量化参数。

## 训练后量化 Post-training Quantization 

**训练后量化算法(PTQ)** 不需要重新训练，就能将预训练的FP32网络直接转换为定点网络 (如INT8量化)。这个方法通常不需要数据 (即data-free) 或者只需要很少的**校准数据集**，这部分数据一般比较容易获取 (只要大概合适的图片就行，不需要带标签)。由于这种方法几乎不需要调整超参数，可以高效地对预训练的神经网络模型权重进行量化，从而让神经网络量化的应用变得更加广泛。

### 量化范围的设置 Quantization Range Setting

量化范围设置是指确定量化范围的截断阈值 $q_{min}$ 和 $q_{max}$ 的方法。权重通常可以在不需要校准数据的情况下进行量化。然而确定激活量化的参数通常需要几批校准数据 (因为训练完成后模型的权重就固定了，但是激活是和输入有关的)。以下是常见的几种误差函数：

- **绝对值量化**: $V$ 表示要量化的张量。这种方法对异常值很敏感，因为异常值可能会导致过多的舍入误差 (比如最小值比最大值多出一大截，这部分精度就浪费掉了)

$$\begin{align*} 
q_{min} &= min V \\
q_{max} &= max V \tag{11}
\end{align*}$$

- **均方差**: 缓解强离群值问题的一个方法是使用基于MSE的范围设置。在这种范围设置方法中，需要找出 $q_{min}$ 和 $q_{max}$ ，使原始张量和量化张量之间的MSE最小化。其中 $V(q_{min}, q_{max})$ 表示 $V$ 的量化版本, $||~||_{F}^2$ 是 Frobenius 范数 (对于向量Frobenius范数和L2范数相同)。
  
$$\arg min ||V- \hat{V}(q_{min}, q_{max})||^2_F \tag{12}$$

- **交叉熵**(Cross entropy)：对于某些层，被量化的值可能并不重要，比如分类网络最后一层的logits量化。在这种情况下，必须保留量化后的相对大小顺序。这个时候均方差可能不是一个合适的衡量标准，因为它对张量中的所有值进行了同等的权衡，而不管它们的顺序如何。对于大多数的类，通常会得到大量对预测准确性不那么重要的小对数或者是负对数，而有意义的大对数则会少很多。在这种情况下，当均方差试图减少大量小对数误差的时，会对少量的大对数产生较大的量化误差。在这类特定情况下，最小化**交叉熵函数**的值是个更好的方式。 $H(.)$ 是交叉熵函数, $\phi$ 是softmax函数, $V$ 是logits向量。

$$\arg min H(\phi(V), \phi (\hat{V}(q_{min}, q_{max}))) \tag{13}$$

- **基于BN的范围设置**(BN based range setting)：激活量化器的范围设置通常需要一些校准数据。如果一个层具有批归一化操作，则激活的每通道均值和标准差分别等于学习的批归一化偏移和缩放参数。这些值可以用来为激活量化器找到合适的参数。其中 β 和 γ 是每个通道学习的偏移和缩放参数的向量。并且α > 0

$$\begin{align*} 
q_{min} &= min (\beta - \alpha \gamma) \\
q_{max} &= max (\beta + \alpha \gamma) \tag{14}
\end{align*}$$

- **对比**: 在 Table 1 中，比较了权重量化的范围设置方法。对于高位宽，均方差和绝对值量化方法基本持平。然而，在较低的位宽下，均方差方法明显优于绝对值量化。在 Table 2 中，对激活量化进行了类似的比较。可以看到，均方差与最后一层的交叉熵相结合，表示为MSE + Xent，优于其他方法，特别是在较低的位宽下。该表还清楚地表明了对最后一层使用交叉熵而不是均方差的好处。(权重bit用Wx表示，激活bit用Ax表示)

![量化范围表1](/images/Model_Accelaration/Table%201%20for%20range%20setting%20of%20quantizers.png)

![量化范围表2](/images/Model_Accelaration/Table%202%20for%20range%20setting%20of%20quantizers.png)

KL散度

### 逐层量化 Cross-Layer Equalization

导致量化误差的一个常见问题是，**同一张量中的元素可以有明显不同的大小**。量化范围设置试图在截断误差和舍入误差之间找到一个良好的平衡点，但在某些情况下，张量值之间的差异是比较大的，即使是适度的量化(如INT8)也找不到一个合适的折衷方案。

这种情况在深度可分离层中尤其普遍，因为每个输出特征只有几个权重，这可能导致权重的更高可变性。此外，批归一化折叠会增强这种效果，并可能导致连接到各输出通道的权重之间的强烈不均衡 (见图5)。虽然对于细粒度更小的量化粒度 (例如按通道量化) 来说问题不大，但对于更广泛使用的按层量化来说，这仍然是个问题。许多文章使用了深度可分离卷积的模型，例如 [MobileNetV1](/docs/Model_Networks/LightWeight_Networks/MobileNetV1.md) 和 [MobileNetV2](/docs/Model_Networks/LightWeight_Networks/MobileNetV2.md) 这类紧凑网络上，逐层量化效果明显变差，甚至量化完精度就掉没了，主要是因为深度可分离卷积各个通道之间数值范围差异过大的原因。

> **补充**：BN融合到卷积或者全连接的时会改变权重参数，同时因为BN学习了偏移和缩放参数，可能会导致各个通道的差异进一步变大。两个操作分别量化现在被合并到一起量化了，所以可能会加剧通道间的差异

![每通道BN融合的范围](/images/Model_Accelaration/BN%20folding%20per%20channel.png)

[DFQ](/docs/Model_Accelaration/Data_Free/DFQ.md) 和 [Recovering Neural Network Quantization Error Through Weight Factorization](/docs/Model_Accelaration/Data_Free/Recovering%20Neural%20Network%20Quantization%20Error%20Through%20Weight%20Factorization.md) 介绍了一种不使用按通道量化来克服这种不平衡的方法。在这两篇论文中，作者发现对于许多常见的激活函数 (例如ReLU、PreLU)，下面公式的正比例的缩放关系是成立的:

$$f(sx)=sF(x) \tag{15}$$

对于任何非负实数s，这种关系适用于所有一阶齐次函数，并且可以通过缩放参数的扩展支持任何非线性分段函数 (例如Relu6)。可以在神经网络的连续层中应用这种正比例缩放关系。给定两个层

$$\begin{align*}
h &= f(W^{(1)}x+b^{(1)}) \\
y &= f(W^{(2)}h+b^{(2)}) \tag{16}
\end{align*}$$

通过缩放等价性，可以看到

$$\begin{align*}
y &= f(W^{(2)}f(W^{(1)}x+b^{(1)})+b^{(2)}) \\
&= f(W^{(2)}S\hat{f}(S^{-1}W^{(1)}x + S^{-1}b^{(1)})+b^{(2)}) \\
&= f(\widetilde{W}^{(2)}\hat{f}(\widetilde{W}^{(1)}x+\widetilde{b}^{(1)})+b^{(2)}) \tag{17} \\
\end{align*}$$

$$\widetilde{W}^{(2)}=W^{(2)}S, W^{(1)}=S^{-1}W^{(1)}, b^{(1)}=S^{-1}b^{(1)}$$

其中 $S = diag(s)$ 是一个对角矩阵，其值为 $S_{ii}$ 表示神经元 i 的缩放因子 $s_i$。即通过给每个权重每个输出通道除以一个缩放因子来改变每个输出通道的数值范围，使得整个卷积层各个输出通道的权重都落入一个统一合适的数值范围，有效缓解各个通道之间数值差异过大的问题。在 CNN 中按通道进行缩放，并在空间维度上相应地广播，如图六所示：

![通道缩放](/images/Model_Accelaration/Rescaling%20the%20channel%20for%20Data_Free%20quantizers.png)

为了使模型对量化更加具有稳定性，可以找到一个缩放因子 $s_i$ 使得重新缩放层中的量化噪声最小。跨层均衡化通过均衡连续层的动态范围来实现这个过程。论文证明了通过设置参数 $S$ 可以实现最优的权重均衡，使得

$$s_i = \frac{1}{r_i^{(2)}} \sqrt{r_i^{(1)}r_i^{(2)}}$$

其中 $r^{(j)}_i$ 是权重张量 $j$ 的通道 $i$ 的动态范围。 [Recovering Neural Network Quantization Error Through Weight Factorization](/docs/Model_Accelaration/Data_Free/Recovering%20Neural%20Network%20Quantization%20Error%20Through%20Weight%20Factorization.md) 引入了一个类似的比例因子，它也考虑了中间激活张量。但是没有证明这种方法的最优性。

**Absorbing high biases 吸收输入量化的偏移项**

在某些情况下，尤其是在逐层量化之后，高偏差会导致激活的动态范围不同。因此，[DFQ](/docs/Model_Accelaration/Data_Free/DFQ.md) 将高偏差吸收到下一层。为了将第一层的 c（后跟一个 ReLU 激活函数 f）吸收到第二层，可以进行以下重参数化:

$$\begin{align*}
y &= W^{(2)}h+b^{(2)} \\
&= W^{(2)}f(W^{(1)}x+b^{(1)}+c-c)+b^{(2)} \\
&= W^{(2)}(f(W^{(1)}x + \widetilde{b}^{(1)})+c)+b^{(2)} \\
&= \widetilde{W}^{(2)}\widetilde{h}+b^{(2)} \tag{18} \\
\end{align*}$$

$$b^{(2)}=W^{(2)}c+b^{(2)}, \widetilde{h}=h-c, \widetilde{b}^{(1)}=b^{(1)}-c$$

对于具有 ReLU 函数的层，有一个非负向量c，使得 $r(Wx + b -c) = r(Wx + b) -c$ 。平凡解 c = 0 适用于所有 x。然而，根据 x 的分布以及 W 和 b 的值，可能存在一些 $c_i$ > 0 的值，对于经验分布中的（几乎）所有 x，这种等式成立。这个值等于

$$ci = max(0, \underset{x}{min}(W_i^{(1)}x + b_i^{(1)})) \tag{19}$$

其中 $\underset{x}{min}$ 是在一个小的校准数据集上获得的。为了消除对数据的依赖性，论文建议通过批归一化层的偏移和缩放参数来估计(公式17)的右边，其结果是 $ci = max(0,\beta i -3 \gamma i)$。图三是实验结果

![跨层均衡化](/images/Model_Accelaration/Results%20of%20Cross-Layer%20Equalization.png)

### 偏差矫正 Bias Correction

另一个常见问题是，量化误差通常是有偏差，即原始网络的预期输出和量化的层，或者网络的输出是有偏差的 $(E[Wx] ≠ E[\widehat{W}x])$。这种偏差在每个输出通道只有几个元素 (例如3x3卷积核，只有9个元素) 的深度可分离卷积层中更加明显。导致这一误差的主要因素通常是截断误差，因为少数截断过大 (离截断值太远的点) 的异常值可能导致预期分布的改变。

一些文章注意到了这个问题并介绍了纠正预期分布变化的方法。对于量化误差 $\Delta W = \widehat{W} - W$ 的量化层，预期输出分布为

$$\begin{align*}
E[\widehat{y}] &= E[\widehat{W}x] \\
&= E[(W + \Delta W)x] \\
&= E[Wx] + E[\Delta Wx] \tag{20} 
\end{align*}
$$

偏差由 $E[\Delta Wx]$ 给出，由于 $\Delta W$ 是常数，有 $E[\Delta Wx] = \Delta WE[x]$。在 $\Delta WE[x]$ 非零的情况下，输出分布会发生偏移。为了抵消这种偏移，可以从输出中减去它

$$E[y_{corr}] = E[\widehat{W}x] - \Delta WE[x] =E[y] \tag{21} $$

此校正项是与偏置项具有相同形状的向量，因此可以在推理时被吸收到偏置中而无需任何额外开销。

计算偏差校正项有几种方法，其中最常见的两种是**经验偏差校正**和**分析偏差校正**(empirical bias correction and analytic bias correction)。

- **经验偏差校正**: 使用校准数据集，偏差校正项可以简单地通过比较量化模型和全精度模型的激活来计算，可以通过计算以下内容逐层完成，参考公式17

$$\Delta WE[x] = E[\widehat{W}x] - E[Wx] \tag{22}$$

- **分析偏差校正**: [DFQ](/docs/Model_Accelaration/Data_Free/DFQ.md) 引入一种不需要数据的分析计算偏差误差的方法。对于具有批归一化和 ReLU 函数的常见网络，使用前一层的 BN 统计量来计算预期的输入分布 $E[x]$。BN 参数 γ 和 β 对应于 BN 层输出的均值和标准差。假设输入值是正态分布，ReLU 对分布的影响可以使用截断正态分布来建模

$$\begin{align*}
E[x] &= E[ReLU(x^{pre})] \\
&= \gamma N(\frac{-\beta}{\gamma}) + \beta [1 - \Phi(\frac{-\beta}{\gamma})] \tag{23}
\end{align*}
$$

其中 $x^{pre}$ 是预激活(上一层)输出，假定其为正态分布，每个通道的平均值为 $\beta$，每个通道的标准差为 $\gamma$, $\Phi(-)$ 是标准正态累积分布函数 (概率密度函数的积分)，符号 $N(x)$ 表示标准正态概率密度函数。所有向量的操作都是逐元素进行(按通道 per-channel，意思是每个通道用一组值进行操作)。在计算了输入分布 $E[x]$ 之后，修正项可以通过与权重量化误差 $\Delta W$ 相乘得到 (公式19 $\Delta W$ 是固定的，因为量化不会修改原本权重的信息，所以公式19是非常容易计算出来的)。图4是偏差校正将 MobileNetV2 量化为 8 位的结果。

![偏差矫正算法对比](/images/Model_Accelaration/Impact%20of%20bias%20correction%20for%20MobileNetV2.png)

### 自适应取整 AdaRound

神经网络的权重通常是通过将每个浮点数值映射到最近的量化网格点来进行量化的，如公式(4)，一般把这种量化策略称为**四舍五入**。四舍五入方法对于一个固定的量化网格，其在浮点数权重和量化后权重之间产生的均方误差最小。然而，四舍五入法在训练后量化权重时并不是最好的选择。下图是100个不同的随机舍入样本将 ResNet18 第一层的权重量化为4比特，并评估了每个舍入选择的网络性能。其中最好的舍入选择比四舍五入的表现要好10%以上。

![AdaRound Figure 7](/images/Model_Accelaration/AdaRound%20Figure%207.png)

[AdaRound]() 可以为训练后量化提供更好的权重取舍。AdaRound是一种理论上有根据的、计算上有效的方法，在实践中显示出显著的性能改进。

## 量化算法

### 二值量化

A Review of Binarized Neural Networks


https://arxiv.org/abs/2110.06804

https://arxiv.org/abs/1605.04711

https://arxiv.org/abs/1602.02830

https://arxiv.org/abs/1603.05279

IR-Net: Forward and Backward Information Retention for Highly Accurate Binary Neural Networks

DMS: Differentiable diMension Search for Binary Neural Networks

### INT8 量化

Towards Unified INT8 Training for Convolutional Neural Network

8-bit-inference-with-tensorrt CSDN 知乎

https://arleyzhang.github.io/articles/923e2c40/

https://note.youdao.com/ynoteshare/index.html?id=829ba6cabfde990e2832b048a4f492b3&type=note&_time=1711435479349#/

https://zhuanlan.zhihu.com/p/405571578

https://arxiv.org/abs/2004.09602

https://zhuanlan.zhihu.com/p/509353790

https://zhuanlan.zhihu.com/p/38328685?utm_campaign=shareopn&utm_medium=social&utm_oi=944480243856162816&utm_psn=1573017689664139264&utm_source=wechat_session

### 任意bit量化

### FP8量化

https://zhuanlan.zhihu.com/p/565021881

https://www.qinglite.cn/doc/164964781e4aa7db6

[FP8量化](/docs/Model_Accelaration/FP8量化.md) 

## 实际应用 Practical Considerations

在对神经网络进行量化时，会面临很多量化方式的选择，包括量化方案、细粒度和位宽。

### 对称和非对称量化 Symmetric vs. Asymmetric Quantization

对于所有的权重和输入，非对称量化可以有更好的表达能力，因为使用了相对零点这个参数，但会使计算量更大。参考公式(3)，非对称量化的权重和输入相乘时:

$$\begin{align*} 
\widehat{W} \cdot \widehat{x} &= s_W(W_i-z_W)s_x(x_i-z_x) \\
&= s_W s_x W_i x_i - s_W z_W s_x x_i -s_W s_x z_x W_i + s_W z_W s_x z_x \tag{8}
\end{align*}$$

权重和输入都采用对称量化时，即不使用相对零点，则只有第一项。使用非对称量化时，第三和第四项只取决于比例 $s$、零点 $z$ 和权重值，因此这两个部分可以提前计算出结果，添加到对应层的偏置中，在推理时不需要额外计算。第二项取决于输入数据 $x$，每次计算都会导致更大的延迟和功耗开销。因此，对输入使用非对称量化，对权重使用对称量化是一种比较常见的方法，这可以避免额外的数据依赖和计算量。

### 按层量化还是按通道量化 Per-tensor and Per-channel Quantization

前文提到的量化粒度中，按层量化因为所有加速器都支持这种量化方法得到广泛应用。然而权重分布在各个通道之间差异比较大的时候，按通道量化可以提高准确性。如公式(3)，对权重按通道量化可以通过对每个权重通道使用不同的缩放系数来实现，而不需要对整体进行缩放操作。对输入按通道量化则很难实现，因为无法将缩放系数从求和中简单的分解出来，因此需要对每个输入加法器的数据通道进行单独的重新调整，故**对输入一般采用逐层量化**。虽然权重按通道量化越来越普遍，但并不是所有的加速器硬件都支持这种操作。

总之，激活层的量化使用按层量化就已经足够，卷积或者反卷积使用按通道量化。

### 量化精度 Quantization Precision

部署量化的神经网络模型有两种常见方法，即**模拟量化（也称为虚假量化）** 和 **仅整数量化（也称为定点量化）**。

- **模拟量化**：模型参数以低精度存储，但操作（例如矩阵乘法和卷积）使用浮点运算执行，因此在执行浮点操作之前，需要对量化参数进行反量化，如图 6 中
- **定点量化**：所有操作都使用低精度整数算术进行执行，如图 6 右。实际量化时一般使用定点量化
- **全精度推理**：即不使用量化，保留FP32的精度，如图 6 左
- **混合精度量化**：使用低精度量化时，提高了推理速度但是推理精度下降了，则可以通过混合精度量化来解决这个问题。如图 8 所示，每层都以不同的精度进行量化。这种方法的问题是如何为每一层选择什么混合精度，是一个搜索问题，通常使用 [NAS](/docs/Model_Accelaration/网络搜索.md) 方法进行解决。

![Quantization Precision](/images/Model_Accelaration/Quantization%20Precision.png)

![Illustration of mixed-precision quantization](/images/Model_Accelaration/Illustration%20of%20mixed-precision%20quantization.png)

### 量化模型的落地 Quantitative Model Implementation

虽然学术界大家很早就开始做量化，但现在算法还无法大规模落地，核心是**精度问题**。主要存在问题：

**精度挑战大**
- 线性量化对分布的描述不精确
- 低精度量化: 比特数越低，精度损失越大，实用性就越差
- 分类 to 检测 to 识别: 任务越难，精度损失越大，比如识别任务，就比分类任务要难非常多
- 大模型 to 小模型: 通常认为模型越小，精度损失越大
- 某些特定结构，如深度可分离卷积对量化精度不友好
- 常见的对部署友好的方法比如 BN 层融合，全量化 (Fully Quantized)，都会给精度带来更大的挑战

**软硬件支持程度**
- 不同硬件高效支持的低比特指令不一样，同样训练得到的低比特模型，无法直接部署在所有硬件上
- 不同软件库提供的量化方案/量化细节不一样：量化细节里包括量化位置、是否支持逐通道量化、是否支持混合精度等等。即使硬件支持了量化，但不是所有硬件都可以在低比特上提供更好的速度提升。造成这个状况的主要原因有多个，一方面是指令集峰值提升可能本身就并不多，而要引入较多的额外计算，另一方面也取决于软件工程师优化指令的水平，同时由于网络结构灵活多样，不一定能在不同网络结构上达到同样好的加速比，需要优化足够多的的corner case才可以解决。

### 量化模型框架 Tools of Quantization

不同量化训练工具能部署的推理框架和平台，是不一样的。如果要把量化的模型部署到硬件平台上，一般来说，需要先将训练好的 Pytorch 模型转换为中间件模型 ONNX (pth -> onnx)，然后再转换到其他硬件平台框架上。由于量化这块缺乏统一的标准，各个芯片产商都在抢夺市场，各自为战，因此具体到量化推理框架上，各家的算法实现也有很大差异。此外，量化推理和硬件本身有很强的关联性，不同芯片由于支持的指令集和硬件单元的差异，也会导致量化算法有一些差别。

| 公司   | 量化工具        | 推理引擎    | 部署平台                |
|--------|-----------------|------------|-----------------------|
| Intel  | NNCF            | OpenVino   | x86 CPU               |
| 高通   | AIMet           | SNPE/QNN   | 高通 DSP/NPU 等芯片   |
| MTK    | Neuropilot      | Neuron SDK | 联发科 APU 芯片       |
| Nvidia | TensorRT（TRT） | TensorRT   | 英伟达部分 GPU 芯片   |
| 商汤   | MQBench、PPL    | PPL        | --                    |
| 腾讯   | NCNN            | NCNN       | 多种 CPU 平台         |
| Meta   | PyTorch         | Libtorch   | ARM/x86 CPU           |
| 微软   | NNI             | --         | --                    |
| 华为   | MindSpore Lite converter—PTQ <br> MindSpore Lite—QAT <br> MindSpore | --  | 端、边、云 |
| tvm团队| TVM             | TVM        | 端、边、云            |
| 百度   | PaddleSlim      | --         | --                    |
| 瑞芯微   | RKNN      | --         | 瑞芯微 RK3588、RK3399pro   |

















































对比 PTQ(static) 和 QAT，其主要区别在于是否进行重新训练，PTQ 只需要少量校准数据，流程简单，而 QAT 需要插入伪量化节点重新训练/微调，流程更复杂，但精度通常也更高。


![Comparison between QAT and Post-Training Quantization PTQ](/images/Model_Accelaration/Comparison%20between%20QAT%20and%20Post-Training%20Quantization%20PTQ.png)



## 参考引用 Reference

### 论文地址 Papers

- [A White Paper on Neural Network Quantization](https://arxiv.org/abs/2106.08295)
- [A Survey of Quantization Methods for Efficient Neural Network Inference](https://arxiv.org/abs/2103.13630)
- [8-bit Inference with TensorRT](https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf)


### 博客 Blogs

- [【已完结】量化神经网络白皮书 2021_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1vf4y1K7km/?spm_id_from=333.337.search-card.all.click&vd_source=08884e3fd559404f507c1ef81f909bfa)
- [再读《神经网络量化白皮书》- 0x01 摘要和绪论 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/462976274)
- [再读《神经网络量化白皮书》- 0x02 量化的一些基础知识 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/462990610)
- [再读《神经网络量化白皮书》- 0x03 训练后量化(PTQ) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/463198140)
- [再读《神经网络量化白皮书》- 0x04 训练时量化(QAT) - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/467841404)
- [再读《神经网络量化白皮书》- 0x05 总结和结论 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/468124540)
- [再读《神经网络量化白皮书》- 0x06 自己的碎碎念 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/468133773)
- [深度学习算法优化系列十一 | 折叠Batch Normalization](https://zhuanlan.zhihu.com/p/107913057)
- [模型量化论文阅读#1----综述：A Survey of Quantization Methods for Efficient Neural Network Inference](https://blog.csdn.net/qq_30614451/article/details/117075048)
- [训练后量化](https://www.yuque.com/yahei/hey-yahei/quantization-post_training#12c5j)
- [模型量化了解一下？ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/132561405)
- [量化知识-1：量化基本知识点梳理 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/557859725)